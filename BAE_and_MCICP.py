# -*- coding: utf-8 -*-
"""Copy of Code_29_8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NJGYg4paJarQpO8KoEOV0kbY1OrdRKJx

# Set up

## Importing libraries
"""

from google.colab import drive
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import nibabel as nib
import cv2, re, itertools,pathlib
from skimage import transform
import scipy.ndimage as ndi
from keras.models import Model
from keras.layers import Input, Dense, Flatten, Conv3D, MaxPooling3D, concatenate
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from sklearn.model_selection import KFold
from sklearn import metrics
from sklearn.metrics import r2_score
from keras import metrics
import random

from keras.models import Model
from keras.layers import Input, Dense, Flatten, Conv3D, MaxPooling3D, concatenate
from keras.models import load_model
from sklearn.model_selection import StratifiedKFold
from keras.callbacks import ReduceLROnPlateau
import keras.backend as K
from tensorflow.keras.initializers import GlorotUniform
from keras.optimizers import Adam
from keras.backend import dropout
import time
from sklearn.model_selection import cross_val_score
import traceback

from numpy.random import seed
seed(42)
# Set the random seed
tf.random.set_seed(42)

"""## Mounting google drive to access the data"""

drive.mount('/content/drive') #mounting drive

"""## Loading in CSV with Image ID, Age, Sex"""

cols = ['Image Data ID', 'Subject', 'Group', 'Sex', 'Age', 'Visit',
       'Description', 'Type', 'Acq Date'] # we dont need all the columns
df = pd.read_csv('/content/drive/MyDrive/CN FOR BAE DATA/Tx_Origin_SpatNORM_CN.csv', usecols = cols)
df.columns = ['ID', 'Subject', 'Group', 'Sex', 'Age', 'Visit',
       'Descrip', 'Type', 'Acq Date'] # renaming cols

df.head()

"""Collecting file paths with the images"""

directory = '/content/drive/MyDrive/CN FOR BAE DATA/Tx_Origin_SpatNORM_CN_imageID' # enter the dir were the images are stored
image_ids = []
for filename in os.listdir(directory):
  image_ids.append(filename)
print(len(image_ids))

df = df[df['ID'].isin(image_ids)] #filterring the df so it only contains the data we need
df = df.reset_index()

df.shape

"""Adding the image paths to the df for the corresponding image ID"""

# new colunm for the paths
df['Loc'] = ''
# iterate over files in that directory
for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    image_id_folder = filename

    for image in os.listdir(f):
      f = os.path.join(f, image)
      if os.path.isfile(f) and f.endswith('nii'):  # checking if it is a file and a .nii file
          df.loc[df['ID']==image_id_folder, 'Loc' ] = f # appending the path

df['Sex'] = df['Sex'].apply(lambda x: 1 if x == 'M' else -1)

"""# Data Information (vis)"""

sex_counts = df["Sex"].value_counts()
# Plot the bar chart
plt.bar(sex_counts.index, sex_counts.values)
plt.xlabel("Sex")
plt.ylabel("Number of Individuals")
plt.title("Number of Males and Females")
plt.show()

plt.hist(df["Age"], bins=20, edgecolor="black")
plt.xlabel("Age")
plt.ylabel("Frequency")
num_ticks = 11  # The number of ticks you want
x_ticks = np.linspace(60, 90, num_ticks)
plt.xticks(x_ticks, fontsize=8)

plt.title("Age Distribution for CN data")

len(df['Subject'].unique())

"""Each subject has only 1 image in the df

No reshaping needed as all are the same shape! This is because the same processing steps were taken by ADNI for each image.

Furthermore each image is 3d not 4d, so t has been average through the image in processing
"""

x = list(df['Loc'])[11] # getting a random image path
brain_vol = nib.load(x)
# What is the type of this object?
print(type(brain_vol))
brain_vol_data = brain_vol.get_fdata()
print(type(brain_vol_data))
plt.imshow(brain_vol_data[60], cmap='bone')
plt.axis('off')
plt.show()

"""# BAE

## Preparing the data
"""

from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout
import keras
from keras.models import load_model
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

"""Extracting data in the correct format for the model"""

# Extract image data and corresponding ages from the DataFrame
image_paths = df['Loc'].values
ages = df['Age'].values
sexes = df['Sex'].values
ids_ = df['ID'].values

# Initialize empty lists to store image arrays and labels
image_data = []

# Loop through each image path, load the image, and append to the lists
for path, age in zip(image_paths, ages):
    image = nib.load(path).get_fdata()
    image_data.append(image)

# Convert lists to numpy arrays
image_data = np.array(image_data)

# Reshape the image data to match the CNN input shape (128, 128, 82, 6) -> (samples, 128, 128, 82, 6)
image_data = image_data.reshape(-1, 79, 95, 68, 1)

train_idx, test_idx, bae_3d_sexes_train, bae_3d_sexes_test = train_test_split(np.arange(102), sexes, test_size=0.2, stratify=sexes)

count = 0
locations = []
shape = image_data.shape
for a in range(shape[0]):
  for b in range(shape[1]):
    for c in range(shape[2]):
      for d in range(shape[3]):
        if pd.isna(image_data[a][b][c][d]):
          locations.append([a,b,c,d])
          count+=1
print(count)

from collections import Counter
Counter([ele[0] for ele in locations])

"""images 10 and 60 have some nans, as this is only 1% of the image we can replace the pixels with averages"""

x = list(df['Loc'])[10] # getting a random image path
brain_vol = nib.load(x)
# What is the type of this object?
type(brain_vol)

brain_vol_data = brain_vol.get_fdata()
type(brain_vol_data)

plt.imshow(brain_vol_data[50], cmap='bone')
plt.axis('off')
plt.show()

"""we can see the nans on the right hand side of the image"""

for location in locations:
  surrounding_pixels = []
  try:
    surrounding_pixels.append(image_data[location[0]][location[1]-1][location[2]][location[3]])
  except:
    None
  try:
    surrounding_pixels.append(image_data[location[0]][location[1]+1][location[2]][location[3]])
  except:
    None
  try:
    surrounding_pixels.append(image_data[location[0]][location[1]][location[2]-1][location[3]])
  except:
    None
  try:
    surrounding_pixels.append(image_data[location[0]][location[1]][location[2]+1][location[3]])
  except:
    None
  try:
    surrounding_pixels.append(image_data[location[0]][location[1]][location[2]][location[3]-1])
  except:
    None
  try:
    surrounding_pixels.append(image_data[location[0]][location[1]][location[2]][location[3]+1])
  except:
    None
  image_data[location[0]][location[1]][location[2]][location[3]]  =  np.nanmean(surrounding_pixels)

"""count = 0
shape = image_data.shape
for a in range(shape[0]):
  for b in range(shape[1]):
    for c in range(shape[2]):
      for d in range(shape[3]):
        if pd.isna(image_data[a][b][c][d]):
          locations.append([a,b,c,d])
          count+=1
print(count)"""

plt.imshow(image_data[10][50], cmap='bone')
plt.axis('off')
plt.show()

"""th e image data now have no Nans, i have filled in the nana by looking at pixels around the nan"""

image_data.shape

bae_3d_im_train, bae_3d_im_test = image_data[train_idx], image_data[test_idx]
bae_3d_ages_train, bae_3d_ages_test = ages[train_idx], ages[test_idx]

"""## Preparing extended CN dataset"""

cols = ['Image Data ID', 'Subject', 'Group', 'Sex', 'Age', 'Visit',
       'Description', 'Type', 'Acq Date'] # we dont need all the columns
df = pd.read_csv('/content/drive/MyDrive/CN FOR BAE DATA/Tx_Origin_SpatNORM_CN.csv', usecols = cols)
df.columns = ['ID', 'Subject', 'Group', 'Sex', 'Age', 'Visit',
       'Descrip', 'Type', 'Acq Date'] # renaming cols

cols = ['Image Data ID', 'Subject', 'Group', 'Sex', 'Age', 'Visit',
       'Description', 'Type', 'Acq Date'] # we dont need all the columns
extended_df = pd.read_csv('/content/drive/MyDrive/CN FOR BAE DATA/CN_not_BL.csv', usecols = cols)
extended_df.columns = ['ID', 'Subject', 'Group', 'Sex', 'Age', 'Visit',
       'Descrip', 'Type', 'Acq Date'] # renaming cols

extended_df.head()

"""Collecting file paths with the images"""

ex_directory = '/content/drive/MyDrive/CN FOR BAE DATA/CN_not_BL' # enter the dir were the images are stored
ex_image_ids = []
for filename in os.listdir(ex_directory):
  ex_image_ids.append(filename)
print(len(ex_image_ids))

extended_df = extended_df[extended_df['ID'].isin(ex_image_ids)] #filterring the df so it only contains the data we need
extended_df = extended_df.reset_index()

extended_df.shape

"""Adding the image paths to the df for the corresponding image ID"""

# new colunm for the paths
extended_df['Loc'] = ''
# iterate over files in that directory
for filename in os.listdir(ex_directory):
    f = os.path.join(ex_directory, filename)
    image_id_folder = filename

    for image in os.listdir(f):
      f = os.path.join(f, image)
      if os.path.isfile(f) and f.endswith('nii'):  # checking if it is a file and a .nii file
          extended_df.loc[extended_df['ID']==image_id_folder, 'Loc' ] = f # appending the path

extended_df['Sex'] = extended_df['Sex'].apply(lambda x: 1 if x == 'M' else -1)

ex_image_data.shape

bae_3d_im_test.shape

# Extract image data and corresponding ages from the DataFrame
ex_image_paths = extended_df['Loc'].values
ex_ages = extended_df['Age'].values
ex_sexes = extended_df['Sex'].values
ex_ids = extended_df['ID'].values

# Initialize empty lists to store image arrays and labels
ex_image_data = []

# Loop through each image path, load the image, and append to the lists
for path, age in zip(ex_image_paths, ex_ages):
    image = nib.load(path).get_fdata()
    ex_image_data.append(image)

# Convert lists to numpy arrays
ex_image_data = np.array(ex_image_data)

# Reshape the image data to match the CNN input shape (128, 128, 82, 6) -> (samples, 128, 128, 82, 6)
ex_image_data = ex_image_data.reshape(-1, 79, 95, 68, 1)

cropped_data = ex_image_data[:, :, :, :68]
print(cropped_data.shape)

ex_image_data_ = cropped_data[..., np.newaxis]

ex_image_data_ = np.array(ex_image_data_)

count = 0
locations = []
shape = ex_image_data_.shape
for a in range(shape[0]):
  for b in range(shape[1]):
    for c in range(shape[2]):
      for d in range(shape[3]):
        if pd.isna(ex_image_data_[a][b][c][d]):
          locations.append([a,b,c,d])
          count+=1
print(count)

from collections import Counter
Counter([ele[0] for ele in locations])

x = list(extended_df['Loc'])[42] # getting a random image path
brain_vol = nib.load(x)
brain_vol_data = brain_vol.get_fdata()
plt.imshow(brain_vol_data[50], cmap='bone')
plt.axis('off')
plt.show()

"""we can see the nans on the right hand side of the image"""

for location in locations:
  surrounding_pixels = []
  try:
    surrounding_pixels.append(ex_image_data_[location[0]][location[1]-1][location[2]][location[3]])
  except:
    None
  try:
    surrounding_pixels.append(ex_image_data_[location[0]][location[1]+1][location[2]][location[3]])
  except:
    None
  try:
    surrounding_pixels.append(ex_image_data_[location[0]][location[1]][location[2]-1][location[3]])
  except:
    None
  try:
    surrounding_pixels.append(ex_image_data_[location[0]][location[1]][location[2]+1][location[3]])
  except:
    None
  try:
    surrounding_pixels.append(ex_image_data_[location[0]][location[1]][location[2]][location[3]-1])
  except:
    None
  try:
    surrounding_pixels.append(ex_image_data_[location[0]][location[1]][location[2]][location[3]+1])
  except:
    None
  ex_image_data_[location[0]][location[1]][location[2]][location[3]]  =  np.nanmean(surrounding_pixels)

count = 0
shape = ex_image_data_.shape
for a in range(shape[0]):
  for b in range(shape[1]):
    for c in range(shape[2]):
      for d in range(shape[3]):
        if pd.isna(ex_image_data_[a][b][c][d]):
          locations.append([a,b,c,d])
          count+=1
print(count)

"""th e image data now have no Nans, i have filled in the nana by looking at pixels around the nan"""

print(ex_image_data_.shape, image_data.shape)
print(ex_sexes.shape, sexes.shape)
print(ex_ages.shape, ages.shape)

plt.imshow(ex_image_data_[42][50], cmap='bone')
plt.axis('off')
plt.show()

plt.imshow(image_data[42][50], cmap='bone')
plt.axis('off')
plt.show()

full_image_data = np.concatenate((ex_image_data_,  image_data))
full_sexes = np.concatenate((ex_sexes, sexes))
full_ages = np.concatenate((ex_ages, ages))

all_ids = set(ex_ids) | set(ids_)
all_ids = np.array(list(all_ids))

int(len(all_ids) * 0.2)

random.shuffle(all_ids)

test_ids = all_ids[:83]
train_ids = all_ids[83:]

train_idxs_bl = [i for i,id in enumerate(ids_) if id in train_ids]
train_idxs_ex  = [i for i,id in enumerate(ex_ids) if id in train_ids]
test_idxs_bl  = [i for i,id in enumerate(ids_) if id in test_ids]
test_idxs_ex  = [i for i,id in enumerate(ex_ids) if id in test_ids]



image_data[train_idxs_bl].shape

bae_3d_im__train, bae_3d_sex__train, bae_3d_age__train = image_data[train_idxs_bl], sexes[train_idxs_bl], ages[train_idxs_bl]
ex_bae_3d_im__train, ex_bae_3d_sex__train, ex_bae_3d_age__train = ex_image_data_[train_idxs_ex], ex_sexes[train_idxs_ex], ex_ages[train_idxs_ex]
bae_3d_im__test, bae_3d_sex__test, bae_3d_age__test = image_data[test_idxs_bl], ages[test_idxs_bl], sexes[test_idxs_bl]
ex_bae_3d_im__test, ex_bae_3d_sex__test, ex_bae_3d_age__test = ex_image_data_[test_idxs_ex], ex_sexes[test_idxs_ex], ex_ages[test_idxs_ex]

full_3d_im_train, full_3d_im_test = full_image_data[train_idx], full_image_data[test_idx]
full_3d_ages_train, full_3d_ages_test = full_ages[train_idx], full_ages[test_idx]

full_3d_im_train, full_3d_im_test = np.concatenate((bae_3d_im__train,ex_bae_3d_im__train)), np.concatenate((bae_3d_im__test,ex_bae_3d_im__test))
full_3d_ages_train, full_3d_ages_test = np.concatenate((bae_3d_age__train, ex_bae_3d_age__train)), np.concatenate((bae_3d_age__test, ex_bae_3d_age__test))
full_3d_sexes_train, full_3d_sexes_test = np.concatenate((bae_3d_sex__train, ex_bae_3d_sex__train)), np.concatenate((bae_3d_sex__test, ex_bae_3d_sex__test))

print(len(full_3d_im_train) == len(full_3d_ages_train))
print(len(full_3d_ages_train) == len(full_3d_sexes_train))

print(len(full_3d_im_test) == len(full_3d_ages_test))
print(len(full_3d_ages_test) == len(full_3d_sexes_test))

"""## Baseline
the baseline will be the average of the ages
"""

mean_age = np.mean(ages)

predictions = np.ones(len(ages))*mean_age


# Calculate different evaluation metrics
mse = mean_squared_error(ages, predictions)
mae = mean_absolute_error(ages, predictions)
r2 = r2_score(ages, predictions)

# Display the metrics
print("Mean Squared Error:", mse)
print("Mean Absolute Error:", mae)
print("R-squared:", r2)

"""## 3D model

### Vanilla model
Adding early stopping to prevent overfitting
"""

model = Sequential() # using sequential

# Add 3D Convolutional layers with MaxPooling
model.add(Conv3D(32, kernel_size=(3, 3, 3), activation='relu', input_shape=(79, 95, 68, 1)))
model.add(MaxPooling3D(pool_size=(2, 2, 2)))
model.add(Conv3D(64, kernel_size=(3, 3, 3), activation='relu'))
model.add(MaxPooling3D(pool_size=(2, 2, 2)))

# Flatten the output and add Dense layers for age prediction
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(1, activation='linear'))  # Use 'linear' activation for regression tasks

# Compile the model
model.compile(loss='mean_squared_error', optimizer='adam')

bae_3d_im_train = np.array(bae_3d_im_train)
bae_3d_im_test = np.array(bae_3d_im_test)
bae_3d_ages_train = np.array(bae_3d_ages_train)
bae_3d_ages_test = np.array(bae_3d_ages_test)

"""Lets train the model with early stopping"""

callbacks = [EarlyStopping(monitor='val_loss', patience=100),
             ModelCheckpoint(filepath='basic_model.h5', monitor='val_loss', save_best_only=True)]

# Run model.fit() and store the training history
history = model.fit(bae_3d_im_train, bae_3d_ages_train, epochs=200, batch_size=16, validation_data=(bae_3d_im_test, bae_3d_ages_test), callbacks = callbacks)

# Access the loss values from the training history
train_loss = history.history['loss']
val_loss = history.history['val_loss']

# Access the number of epochs
epochs = range(1, len(train_loss) + 1)

# Plot the training and validation curves
plt.plot(epochs, train_loss, 'b', label='Training Loss')
plt.plot(epochs, val_loss, 'r', label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""Zoom in on the graph"""

epochs = range(10, len(train_loss) + 1)

# Plot the training and validation curves
plt.plot(epochs, train_loss[9:], 'b', label='Training Loss')
plt.plot(epochs, val_loss[9:], 'r', label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""We see that the model begins to over fit for high epochs and this is where the early stopping stops the models from training"""

saved_model = load_model('basic_model.h5')
predictions = saved_model.predict(bae_3d_im_test)

predictions = model.predict(bae_3d_im_test)

# Calculate different evaluation metrics
mse = mean_squared_error(bae_3d_ages_test, predictions)
mae = mean_absolute_error(bae_3d_ages_test, predictions)
r2 = r2_score(bae_3d_ages_test, predictions)

# Display the metrics
print("Mean Squared Error:", mse)
print("Mean Absolute Error:", mae)
print("R-squared:", r2)

"""The model has improved a very small amount"""

# Assuming you have your data for predictions, test_labels, and sex_train
# Create a scatter plot
plt.scatter(predictions, test_labels, c=test_sex, cmap='viridis', label='Data points')
plt.plot([min(predictions), max(predictions)], [min(predictions), max(predictions)], c='red', label='Ideal line')

# Add labels and title
plt.xlabel('Predictions')
plt.ylabel('Test Labels')
plt.title('Scatter Plot of Predictions vs. Test Labels')

# Add a legend
plt.legend()

# Show the plot
plt.colorbar()  # Add a colorbar to show the mapping of colors to sex_train values
plt.show()

"""Somehow sex is now more scattered, we can still try adding it as a feature to see if it helps

###  V3
Adding sex as a feature with early stopping
"""

def compile_model():
    # Define the input layer for the 3D Convolutional network
    input_img = Input(shape=(79, 95, 68, 1))

    # Apply 3D Convolutional layers with MaxPooling using the Functional API
    x = Conv3D(32, kernel_size=(3, 3, 3), activation='relu')(input_img)
    x = MaxPooling3D(pool_size=(2, 2, 2))(x)
    x = Conv3D(64, kernel_size=(3, 3, 3), activation='relu')(x)
    x = MaxPooling3D(pool_size=(2, 2, 2))(x)

    # Flatten the output and add Dense layers for age prediction
    x = Flatten()(x)
    x = Dense(128, activation='relu')(x)
    output_img = Dense(16, activation='relu')(x)

    # Define the input layer for the extra feature
    input_extra = Input(shape=(1,))

    # Concatenate the output of the Convolutional network with the extra feature
    merged = concatenate([output_img, input_extra])

    # Add Dense layers for further prediction
    merged = Dense(8, activation='relu')(merged)
    output = Dense(1, activation='linear')(merged)  # Use 'linear' activation for regression tasks

    # Define the model with both inputs and the final output
    model = Model(inputs=[input_img, input_extra], outputs=output)

    # Compile the model
    model.compile(loss='mean_squared_error', optimizer='adam',metrics=[metrics.mean_absolute_error])
    return model

def cross_validate(X, y, z, K = 5):
    scores = []
    histories = []
    r2 = []

    for train, test in StratifiedKFold(n_splits=K, shuffle=True).split(X,z):
        model = compile_model() # compile model
        histories.append(model.fit([X[train], z[train]], y[train],
                                   validation_data = ([X[test], z[test]],y[test]), epochs=200, batch_size=16 # feed in the test data for plotting
                                   ).history)
        scores.append(model.evaluate([X[test], z[test]], y[test], verbose = 0)) # evaluate the test dataset
        r2.append(r2_score(model.predict([X[test], z[test]]), y[test]))
    print(model.summary())
    return scores, histories, r2

scores_112, histories_112, r2_112 = cross_validate(image_data, ages, sexes, K = 5)

# Assuming you have your data for predictions, test_labels, and sex_train
# Create a scatter plot
model = compile_model()
X_train, X_test, y_train, y_test, sex_train, sex_test= train_test_split(image_data, ages, sexes, test_size=0.2)
model.fit([X_train, sex_train], y_train, epochs = 200, batch_size = 16)
predictions = model.predict([X_test, sex_test])

color = {1:'orange',-1:'blue'}
label = {1:'Male',-1:'Female'}
#plt.scatter(predictions, y_test, c=[color[i] for i in sex_test], label='Data points', label = 'a')
plt.plot([60,90], [60,90], c='green', label='x=y')

for g in [-1,1]:
    i = np.where(sex_test == g)
    plt.scatter(predictions[i], y_test[i], c=color[g], label = label[g])

# Add labels and title
plt.xlabel('Predicted Age')
plt.ylabel('Actual Age')
plt.title('Scatter Plot of Predictions vs. Actual Age')

# Add a legend
plt.legend()

# Show the plot
plt.show()

callbacks = [EarlyStopping(monitor='val_loss', patience=100),
             ModelCheckpoint(filepath='basic_sex_model.h5', monitor='val_loss', save_best_only=True)]

# Run model.fit() and store the training history
history = model.fit([train_data, train_sex], train_labels, epochs=100, batch_size=16, validation_split=0.2, callbacks = callbacks)

# Access the loss values from the training history
train_loss = history.history['loss']
val_loss = history.history['val_loss']

# Access the number of epochs
epochs = range(1, len(train_loss) + 1)

# Plot the training and validation curves
plt.plot(epochs, train_loss, 'b', label='Training Loss')
plt.plot(epochs, val_loss, 'r', label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

epochs = range(20, len(train_loss) + 1)

# Plot the training and validation curves
plt.plot(epochs, train_loss[19:], 'b', label='Training Loss')
plt.plot(epochs, val_loss[19:], 'r', label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

saved_model = load_model('basic_sex_model.h5')
train_acc = saved_model.evaluate([train_data, train_sex], train_labels, verbose=0)
test_acc = saved_model.evaluate([test_data, test_sex], test_labels, verbose=0)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))

predictions = saved_model.predict([test_data, test_sex])
# Calculate different evaluation metrics
mse = mean_squared_error(test_labels, predictions)
mae = mean_absolute_error(test_labels, predictions)
r2 = r2_score(test_labels, predictions)

# Display the metrics
print("Mean Squared Error:", mse)
print("Mean Absolute Error:", mae)
print("R-squared:", r2)

"""The model has improved slightly"""

# Assuming you have your data for predictions, test_labels, and sex_train
# Create a scatter plot
plt.scatter(predictions, test_labels, c=test_sex, cmap='viridis', label='Sex')
plt.plot([min(predictions), max(predictions)], [min(predictions), max(predictions)], c='red', label='Ideal line')

# Add labels and title
plt.xlabel('Predictions')
plt.ylabel('Test Labels')
plt.title('Scatter Plot of Predictions vs. Test Labels')

# Add a legend
plt.legend()

# Show the plot
plt.colorbar()  # Add a colorbar to show the mapping of colors to sex_train values
plt.show()

"""We see that after adding sex the poins are more equally spread however the model still does not perform too well, perhaps tuning the model will help

### V4
Tuning the model with bayesian optimisation, this is more effecient than grid and random search (typically)
"""

from keras.backend import dropout
import time
import optuna
from optuna.samplers import TPESampler
from sklearn.model_selection import cross_val_score
from keras.optimizers import Adam
import traceback
import gc

def create_model(kernel_size, num_layers, neurons1, neurons2, neurons3, learning_rate, dilation_rate, pool_size, dropout = dropout, activation = 'relu', strides=(1,1,1), padding='same'):
  # Define the input layer for the 3D Convolutional network
  input_img = Input(shape=(79, 95, 68, 1))
  x = Conv3D(neurons1, kernel_size=kernel_size, activation=activation, dilation_rate = dilation_rate)(input_img)
  x = MaxPooling3D(pool_size=pool_size)(x)
  x = Dropout(dropout)(x)
  # Apply 3D Convolutional layers with MaxPooling using the Functional API
  for i in range(num_layers):
    try:
      x = Conv3D(neurons1, kernel_size=kernel_size, activation=activation, dilation_rate = dilation_rate)(x)
      x = MaxPooling3D(pool_size=pool_size)(x)
      x = Dropout(dropout)(x)
    except:
      traceback.print_exc()
  # Flatten the output and add Dense layers for age prediction
  x = Flatten()(x)
  x = Dense(neurons2, activation=activation)(x)
  output_img = Dense(128, activation='relu')(x)
  # Define the input layer for the extra feature
  input_extra = Input(shape=(1,))
  # Concatenate the output of the Convolutional network with the extra feature
  merged = concatenate([output_img, input_extra])
  # Add Dense layers for further prediction
  merged = Dense(neurons3, activation=activation)(merged)
  output = Dense(1, activation='linear')(merged)  # Use 'linear' activation for regression tasks
  # Define the model with both inputs and the final output
  model = Model(inputs=[input_img, input_extra], outputs=output)
  optimizer = Adam(learning_rate=learning_rate)
  # Compile the model
  model.compile(loss='mean_squared_error', optimizer=optimizer)
  return model




def objective(trial, X = image_data,y=ages,z=sexes,K = 5):
    global the_best_score
    """return the mse"""
    num_layers = trial.suggest_int('num_layers', low = 1, high = 2, step = 1)
    neurons1 = trial.suggest_int('neurons1', low = 2, high = 64, step = 4)
    neurons2 = trial.suggest_int('neurons2', low = 2, high = 64, step = 4)
    neurons3 = trial.suggest_int('neurons3', low = 2, high = 64, step = 4)
    batch_size = trial.suggest_int('batch_size', low = 8, high = 64, step = 8)
    learning_rate = trial.suggest_float('learning_rate', low = 0.00001, high = 0.01, step = 0.0005)
    dropout = trial.suggest_float('dropout', low = 0.05, high =0.4, step =10)
    kernel_size = trial.suggest_int('kernel_size', low = 2, high =5, step =1)
    dilation_rate = trial.suggest_int('dilation_rate', low = 1, high =3, step =1)
    pool_size = trial.suggest_int('pool_size', low = 1, high =3, step =1)

    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=25, start_from_epoch=50, baseline = 100)
    # random forest classifier object
    scores = []
    best_scores = []
    for train, test in StratifiedKFold(n_splits=K, shuffle=True).split(X,z):
        model = create_model(kernel_size = (kernel_size,kernel_size,kernel_size), num_layers = num_layers, neurons1 = neurons1, neurons2 = neurons2, neurons3 = neurons3, learning_rate = learning_rate, dilation_rate=(dilation_rate,dilation_rate,dilation_rate), pool_size=(pool_size,pool_size,pool_size), dropout=dropout)
        try:
          history = model.fit([X[train], z[train]], y[train],
                                    validation_data = ([X[test], z[test]],y[test]), epochs=125, batch_size=batch_size, callbacks= [callback]).history
          scores.append(model.evaluate([X[test], z[test]], y[test], verbose = 0))# evaluate the test dataset
          best_scores.append(min(history['val_loss']))
        except:
          traceback.print_exc()
          return(2000)
    the_best_score.append(np.mean(best_scores))
    return(np.mean(scores))


the_best_score = []
# create a study (aim to maximize score)
study = optuna.create_study(sampler=TPESampler(), direction='minimize')

# perform hyperparamter tuning (while timing the process)
time_start = time.time()
study.optimize(objective, n_trials=20, gc_after_trial=True)
time_bayesian = time.time() - time_start

columns = ['Number of iterations', 'Iteration Number of Optimal Hyperparamters', 'Score', 'Time Elapsed (s)']

# store result in a data frame
values_bayesian = [100, study.best_trial.number, study.best_trial.value, time_bayesian]
results_bayesian = pd.DataFrame([values_bayesian], columns = columns)

values = [study.trials[i].values[0] for i in range(len(study.trials))]
params = [study.trials[i].params for i in range(len(study.trials))]
columns = list(params[0].keys()) + ['val']
bayesian_results = pd.DataFrame(columns = columns)
bayesian_results['val'] = values
for param in list(params[0].keys()):
  param_list = [study[param] for study in params]
  bayesian_results[param] = param_list

bayesian_results.sort_values(by=['val'])

optuna.visualization.plot_contour(study)

optuna.visualization.plot_param_importances(study)

"""Tuning the model seems to make no real improvement, therefore other approaches must be taken, transfer learning can help as we only have 100 images

### PIP installs for BO and 3D VGG
"""

pip install keras_applications

pip install classification-models-3D

pip install optuna

import optuna
from optuna.samplers import TPESampler

"""
### Version 5 no fine tuning"""

from classification_models_3D.tfkeras import Classifiers

vgg19, preprocess_input = Classifiers.get('vgg19')
pretrained_model = vgg19(input_shape=(79, 95, 68, 3), weights='imagenet')

def compile_model():

  # Define the input layer for the 3D Convolutional network
  model = Sequential()

  for layer in pretrained_model.layers:
      model.add(layer)

  # Freeze the weights of the pre-trained layers so they are not updated during training
  for layer in model.layers:
      layer.trainable = False


  # Flatten the output of the pre-trained model
  x = Flatten()(model.output)

  # Add Dense layers after concatenation
  x = Dense(256, activation='relu')(x)
  x = Dense(64, activation='relu')(x)

  additional_input = Input(shape=(1,))
  # Concatenate the flattened output with the new input
  concatenated = concatenate([x, additional_input])
  output = Dense(1, activation='linear')(concatenated)  # Use 'linear' activation for regression tasks

  # Create the final model
  model = Model(inputs=[pretrained_model.input, additional_input], outputs=output)

  for layer in model.layers[23:]:
            layer.trainable = True

  METRICS = [keras.metrics.MeanAbsoluteError(name='mae')]

  # Compile the model
  model.compile(loss='mean_squared_error', optimizer='adam', metrics=METRICS)

  return(model)

"""As this cnn takes in rgb input we must duplicate each image twice so that the input shapes match"""

duplicated_data = np.repeat(image_data, 3, axis=-1)
print(duplicated_data.shape)

from sklearn.model_selection import StratifiedKFold

def cross_validate(X, y, z, K = 5):
    scores = []
    histories = []
    r2 = []

    for train, test in StratifiedKFold(n_splits=K, shuffle=True).split(X,z):
        model = compile_model() # compile model
        histories.append(model.fit([X[train], z[train]], y[train],
                                   validation_data = ([X[test], z[test]],y[test]), epochs=200, batch_size=16 # feed in the test data for plotting
                                   ).history)
        scores.append(model.evaluate([X[test], z[test]], y[test], verbose = 0)) # evaluate the test dataset
        r2.append(r2_score(model.predict([X[test], z[test]]), y[test]))
    print(model.summary())
    return scores, histories, r2

scores_115, histories_115, r2_115 = cross_validate(duplicated_data, ages, sexes, K = 5)

for history in (histories_115):
  # Access the loss values from the training history
  train_loss = history['loss']
  val_loss = history['val_loss']

  # Access the number of epochs
  epochs = range(1, len(train_loss) + 1)

  # Plot the training and validation curves
  plt.plot(epochs, train_loss, 'b', label='Training Loss')
  plt.plot(epochs, val_loss, 'r', label='Validation Loss')
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.legend()
  plt.show()
  epochs = range(10, len(train_loss) + 1)

  # Plot the training and validation curves
  plt.plot(epochs, train_loss[9:], 'b', label='Training Loss')
  plt.plot(epochs, val_loss[9:], 'r', label='Validation Loss')
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.legend()
  plt.show()

dup_bae_3d_im_train = np.repeat(bae_3d_im_train, 3, axis=-1)
dup_bae_3d_im_test = np.repeat(bae_3d_im_test, 3, axis=-1)

model = compile_model()

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, start_from_epoch=10, baseline = 50)
history = model.fit([dup_bae_3d_im_train, bae_3d_sexes_train], bae_3d_ages_train, validation_data= ([dup_bae_3d_im_test, bae_3d_sexes_test], bae_3d_ages_test), epochs = 100, batch_size = 16, callbacks =[callback]).history

predictions = model.predict([dup_bae_3d_im_test, bae_3d_sexes_test])

color = {1:'orange',-1:'blue'}
label = {1:'Male',-1:'Female'}
#plt.scatter(predictions, y_test, c=[color[i] for i in sex_test], label='Data points', label = 'a')
plt.plot([60,90], [60,90], c='green', label='x=y')

for g in [-1,1]:
    i = np.where(bae_3d_sexes_test == g)
    plt.scatter(predictions[i], bae_3d_ages_test[i], c=color[g], label = label[g])

# Add labels and title
plt.xlabel('Predicted Age')
plt.ylabel('Actual Age')
plt.title('Scatter Plot of Predictions vs. Actual Age')

# Add a legend
plt.legend()

# Show the plot
plt.show()

from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

def evaluate_predictions(predictions, true_values):
    # Calculate R^2
    r2 = r2_score(true_values, predictions)

    # Calculate MSE
    mse = mean_squared_error(true_values, predictions)

    # Calculate RMSE
    rmse = np.sqrt(mse)

    # Calculate MAE
    mae = mean_absolute_error(true_values, predictions)

    return r2, rmse, mse, mae

# Sample data


r2, rmse, mse, mae = evaluate_predictions(predictions, bae_3d_ages_test)
print(f"R^2: {r2}")
print(f"RMSE: {rmse}")
print(f"MSE: {mse}")
print(f"MAE: {mae}")

"""### Version 4 no fine tuning"""

from classification_models_3D.tfkeras import Classifiers

vgg16, preprocess_input = Classifiers.get('vgg16')
pretrained_model = vgg16(input_shape=(79, 95, 68, 3), weights='imagenet')

def compile_model():

  # Define the input layer for the 3D Convolutional network
  model = Sequential()

  for layer in pretrained_model.layers:
      model.add(layer)

  # Freeze the weights of the pre-trained layers so they are not updated during training
  for layer in model.layers:
      layer.trainable = False


  # Flatten the output of the pre-trained model
  x = Flatten()(model.output)

  # Add Dense layers after concatenation
  x = Dense(256, activation='relu')(x)
  x = Dense(64, activation='relu')(x)

  additional_input = Input(shape=(1,))
  # Concatenate the flattened output with the new input
  concatenated = concatenate([x, additional_input])
  output = Dense(1, activation='linear')(concatenated)  # Use 'linear' activation for regression tasks

  # Create the final model
  model = Model(inputs=[pretrained_model.input, additional_input], outputs=output)

  for layer in model.layers[20:]:
            layer.trainable = True

  METRICS = [keras.metrics.MeanAbsoluteError(name='mae')]

  # Compile the model
  model.compile(loss='mean_squared_error', optimizer='adam', metrics=METRICS)

  return(model)

dup_bae_3d_im_train = np.repeat(bae_3d_im_train, 3, axis=-1)
dup_bae_3d_im_test = np.repeat(bae_3d_im_test, 3, axis=-1)

model = compile_model()

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, start_from_epoch=10, baseline = 50)
history = model.fit([dup_bae_3d_im_train, bae_3d_sexes_train], bae_3d_ages_train, validation_data= ([dup_bae_3d_im_test, bae_3d_sexes_test], bae_3d_ages_test), epochs = 200, batch_size = 16, callbacks =[callback]).history

predictions = model.predict([dup_bae_3d_im_test, bae_3d_sexes_test])

color = {1:'orange',-1:'blue'}
label = {1:'Male',-1:'Female'}
#plt.scatter(predictions, y_test, c=[color[i] for i in sex_test], label='Data points', label = 'a')
plt.plot([60,90], [60,90], c='green', label='x=y')

for g in [-1,1]:
    i = np.where(bae_3d_sexes_test == g)
    plt.scatter(predictions[i], bae_3d_ages_test[i], c=color[g], label = label[g])

# Add labels and title
plt.xlabel('Predicted Age')
plt.ylabel('Actual Age')
plt.title('Scatter Plot of Predictions vs. Actual Age')

# Add a legend
plt.legend()

# Show the plot
plt.show()

from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

def evaluate_predictions(predictions, true_values):
    # Calculate R^2
    r2 = r2_score(true_values, predictions)

    # Calculate MSE
    mse = mean_squared_error(true_values, predictions)

    # Calculate RMSE
    rmse = np.sqrt(mse)

    # Calculate MAE
    mae = mean_absolute_error(true_values, predictions)

    return r2, rmse, mse, mae

# Sample data


r2, rmse, mse, mae = evaluate_predictions(predictions, bae_3d_ages_test)
print(f"R^2: {r2}")
print(f"RMSE: {rmse}")
print(f"MSE: {mse}")
print(f"MAE: {mae}")

"""### Version 4 fine-tuning v2"""

from classification_models_3D.tfkeras import Classifiers

vgg16, preprocess_input = Classifiers.get('vgg16')
pretrained_model = vgg16(input_shape=(79, 95, 68, 3), weights='imagenet')

import keras.backend as K
from tensorflow.keras.initializers import GlorotUniform
import random
def compile_model():

  # Define the input layer for the 3D Convolutional network
  model = Sequential()

  for layer in pretrained_model.layers:
      model.add(layer)

  # Freeze the weights of the pre-trained layers so they are not updated during training
  for layer in model.layers:
      layer.trainable = False


  # Flatten the output of the pre-trained model
  x = Flatten()(model.output)

  # Add Dense layers after concatenation
  x = Dense(256, activation='relu')(x)
  x = Dense(64, activation='relu')(x)

  additional_input = Input(shape=(1,))
  # Concatenate the flattened output with the new input
  concatenated = concatenate([x, additional_input])
  output = Dense(1, activation='linear')(concatenated)  # Use 'linear' activation for regression tasks

  # Create the final model
  model = Model(inputs=[pretrained_model.input, additional_input], outputs=output)

  for layer in model.layers[15:]:
            layer.trainable = True

  session = K.get_session()
  for layer in model.layers[15:19]:
      if layer.get_weights():
        weights_shape = layer.get_weights()[0].shape
        bias_shape = layer.get_weights()[1].shape

        # Re-initialize weights and biases
        new_weights = GlorotUniform()(weights_shape).numpy()
        new_biases = GlorotUniform()(bias_shape).numpy()

        layer.set_weights([new_weights, new_biases])




  METRICS = [keras.metrics.MeanAbsoluteError(name='mae')]

  # Compile the model
  model.compile(loss='mean_squared_error', optimizer='adam', metrics=METRICS)

  return(model)

dup_bae_3d_im_train = np.repeat(bae_3d_im_train, 3, axis=-1)
dup_bae_3d_im_test = np.repeat(bae_3d_im_test, 3, axis=-1)

model = compile_model()

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, start_from_epoch=10, baseline = 50)
history = model.fit([dup_bae_3d_im_train, bae_3d_sexes_train], bae_3d_ages_train, validation_data= ([dup_bae_3d_im_test, bae_3d_sexes_test], bae_3d_ages_test), epochs = 200, batch_size = 16, callbacks =[callback]).history

predictions = model.predict([dup_bae_3d_im_test, bae_3d_sexes_test])

for layer in model.layers:
    if layer.trainable:
        print(f"Layer {layer.name} is trainable (not frozen).")
    else:
        print(f"Layer {layer.name} is not trainable (frozen).")

color = {1:'orange',-1:'blue'}
label = {1:'Male',-1:'Female'}
#plt.scatter(predictions, y_test, c=[color[i] for i in sex_test], label='Data points', label = 'a')
plt.plot([60,90], [60,90], c='green', label='x=y')

for g in [-1,1]:
    i = np.where(bae_3d_sexes_test == g)
    plt.scatter(predictions[i], bae_3d_ages_test[i], c=color[g], label = label[g])

# Add labels and title
plt.xlabel('Predicted Age')
plt.ylabel('Actual Age')
plt.title('Scatter Plot of Predictions vs. Actual Age')

# Add a legend
plt.legend()

# Show the plot
plt.show()

from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

def evaluate_predictions(predictions, true_values):
    # Calculate R^2
    r2 = r2_score(true_values, predictions)

    # Calculate MSE
    mse = mean_squared_error(true_values, predictions)

    # Calculate RMSE
    rmse = np.sqrt(mse)

    # Calculate MAE
    mae = mean_absolute_error(true_values, predictions)

    return r2, rmse, mse, mae

# Sample data


r2, rmse, mse, mae = evaluate_predictions(predictions, bae_3d_ages_test)
print(f"R^2: {r2}")
print(f"RMSE: {rmse}")
print(f"MSE: {mse}")
print(f"MAE: {mae}")

"""### Version 4 BO old gen"""

from classification_models_3D.tfkeras import Classifiers

vgg16, preprocess_input = Classifiers.get('vgg16')
pretrained_model = vgg16(input_shape=(79, 95, 68, 3), weights='imagenet')

def create_model(num_ft_layers, neurons1, neurons2):

  # Define the input layer for the 3D Convolutional network
  model = Sequential()

  for layer in pretrained_model.layers:
      model.add(layer)

  # Freeze the weights of the pre-trained layers so they are not updated during training
  for layer in model.layers:
      layer.trainable = False


  # Flatten the output of the pre-trained model
  x = Flatten()(model.output)

  # Add Dense layers after concatenation
  x = Dense(neurons1, activation='relu')(x)

  x = Dense(neurons2, activation='relu')(x)

  additional_input = Input(shape=(1,))
  concatenated = concatenate([x, additional_input])
  # Concatenate the flattened output with the new input

  output = Dense(1, activation='linear')(concatenated)  # Use 'linear' activation for regression tasks

  # Create the final model
  model = Model(inputs=[pretrained_model.input, additional_input], outputs=output)

  for layer in model.layers[len(pretrained_model.layers)- num_ft_layers:]:
            layer.trainable = True

  METRICS = [keras.metrics.MeanAbsoluteError(name='mae')]

  # Compile the model
  model.compile(loss='mean_squared_error', optimizer='adam', metrics=METRICS)

  return(model)

dup_bae_3d_im_train = np.repeat(bae_3d_im_train, 3, axis=-1)
dup_bae_3d_im_test = np.repeat(bae_3d_im_test, 3, axis=-1)

from keras.backend import dropout
import time
import optuna
from optuna.samplers import TPESampler
from sklearn.model_selection import cross_val_score
from keras.optimizers import Adam
import traceback

def objective(trial, X = dup_bae_3d_im_train, y = bae_3d_ages_train, z = bae_3d_sexes_train,K = 3):
    global the_best_score
    """return the mse"""
    num_ft_layers = trial.suggest_int('num_layers', low = 0, high = 8, step = 1)
    neurons1 = trial.suggest_int('neurons1', low = 2, high = 256, step = 16)
    neurons2 = trial.suggest_int('neurons2', low = 2, high = 256, step = 16)
    batch_size = trial.suggest_int('batch_size', low = 8, high = 16, step = 1)


    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, start_from_epoch=10, baseline = 50)
    # random forest classifier object
    scores = []
    best_scores = []
    for train, test in StratifiedKFold(n_splits=K, shuffle=True).split(X,z):
        model = create_model(num_ft_layers = num_ft_layers, neurons1 = neurons1, neurons2 = neurons2)
        try:
          history = model.fit([X[train], z[train]], y[train],
                                    validation_data = ([X[test], z[test]],y[test]), epochs=100, batch_size=batch_size, callbacks= [callback]).history
          scores.append(model.evaluate([X[test], z[test]], y[test], verbose = 0))# evaluate the test dataset
          best_scores.append(min(history['val_loss']))
        except:
          traceback.print_exc()
          return(2000)
    the_best_score.append(np.mean(best_scores))
    return(np.mean(scores))


the_best_score = []
# create a study (aim to maximize score)
study = optuna.create_study(sampler=TPESampler(), direction='minimize')

# perform hyperparamter tuning (while timing the process)
time_start = time.time()
study.optimize(objective, n_trials=5, gc_after_trial=True)
time_bayesian = time.time() - time_start

columns = ['Number of iterations', 'Iteration Number of Optimal Hyperparamters', 'Score', 'Time Elapsed (s)']

# store result in a data frame
values_bayesian = [100, study.best_trial.number, study.best_trial.value, time_bayesian]
results_bayesian = pd.DataFrame([values_bayesian], columns = columns)

values = [study.trials[i].values[0] for i in range(len(study.trials))]
params = [study.trials[i].params for i in range(len(study.trials))]
columns = list(params[0].keys()) + ['val']
bayesian_results = pd.DataFrame(columns = columns)
bayesian_results['val'] = values
for param in list(params[0].keys()):
  param_list = [study[param] for study in params]
  bayesian_results[param] = param_list

bayesian_results.sort_values(by=['val'])

optuna.visualization.plot_param_importances(study)

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, start_from_epoch=10, baseline = 50)
model = create_model(4, 34, 162)
history = model.fit([dup_bae_3d_im_train, bae_3d_sexes_train], bae_3d_ages_train, validation_data= ([dup_bae_3d_im_test, bae_3d_sexes_test], bae_3d_ages_test), epochs = 100, batch_size = 9, callbacks = [callback]).history

predictions = model.predict([dup_bae_3d_im_train, bae_3d_sexes_train])

color = {1:'orange',-1:'blue'}
label = {1:'Male',-1:'Female'}
#plt.scatter(predictions, y_test, c=[color[i] for i in sex_test], label='Data points', label = 'a')
plt.plot([60,90], [60,90], c='green', label='x=y')

for g in [-1,1]:
    i = np.where(bae_3d_sexes_test == g)
    plt.scatter(predictions[i], bae_3d_ages_test[i], c=color[g], label = label[g])

# Add labels and title
plt.xlabel('Predicted Age')
plt.ylabel('Actual Age')
plt.title('Scatter Plot of Predictions vs. Actual Age')

# Add a legend
plt.legend()

# Show the plot
plt.show()

from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

def evaluate_predictions(predictions, true_values):
    # Calculate R^2
    r2 = r2_score(true_values, predictions)

    # Calculate MSE
    mse = mean_squared_error(true_values, predictions)

    # Calculate RMSE
    rmse = np.sqrt(mse)

    # Calculate MAE
    mae = mean_absolute_error(true_values, predictions)

    return r2, rmse, mse, mae

# Sample data


r2, rmse, mse, mae = evaluate_predictions(predictions, bae_3d_ages_test)
print(f"R^2: {r2}")
print(f"RMSE: {rmse}")
print(f"MSE: {mse}")
print(f"MAE: {mae}")

"""### Version 4 bo v2"""

from classification_models_3D.tfkeras import Classifiers

vgg16, preprocess_input = Classifiers.get('vgg16')
pretrained_model = vgg16(input_shape=(79, 95, 68, 3), weights='imagenet')

import keras.backend as K
from tensorflow.keras.initializers import GlorotUniform
from keras.optimizers import Adam

def create_model(num_ft_layers, neurons1, neurons2):

  # Define the input layer for the 3D Convolutional network
  model = Sequential()

  for layer in pretrained_model.layers:
      model.add(layer)

  # Freeze the weights of the pre-trained layers so they are not updated during training
  for layer in model.layers:
      layer.trainable = False


  # Flatten the output of the pre-trained model
  x = Flatten()(model.output)

  # Add Dense layers after concatenation
  x = Dense(neurons1, activation='relu')(x)

  x = Dense(neurons2, activation='relu')(x)

  additional_input = Input(shape=(1,))
  concatenated = concatenate([x, additional_input])
  # Concatenate the flattened output with the new input

  output = Dense(1, activation='linear')(concatenated)  # Use 'linear' activation for regression tasks

  # Create the final model
  model = Model(inputs=[pretrained_model.input, additional_input], outputs=output)

  for layer in model.layers[19 - num_ft_layers:]:
            layer.trainable = True

  session = K.get_session()
  for layer in model.layers[19 - num_ft_layers:19]:
      if layer.get_weights():
        weights_shape = layer.get_weights()[0].shape
        bias_shape = layer.get_weights()[1].shape

        # Re-initialize weights and biases
        new_weights = GlorotUniform()(weights_shape).numpy()
        new_biases = GlorotUniform()(bias_shape).numpy()

        layer.set_weights([new_weights, new_biases])

  METRICS = [keras.metrics.MeanAbsoluteError(name='mae')]


  # Compile the model
  model.compile(loss='mean_squared_error', optimizer='adam', metrics=METRICS)

  return(model)

dup_bae_3d_im_train = np.repeat(bae_3d_im_train, 3, axis=-1)
dup_bae_3d_im_test = np.repeat(bae_3d_im_test, 3, axis=-1)

from keras.backend import dropout
import time
import optuna
from optuna.samplers import TPESampler
from sklearn.model_selection import cross_val_score
from keras.optimizers import Adam
import traceback

def objective(trial, X = dup_bae_3d_im_train, y = bae_3d_ages_train, z = bae_3d_sexes_train,K = 3):
    global the_best_score
    """return the mse"""
    num_ft_layers = trial.suggest_int('num_layers', low = 0, high = 12, step = 2)
    neurons1 = trial.suggest_int('neurons1', low = 2, high = 256, step = 16)
    neurons2 = trial.suggest_int('neurons2', low = 2, high = 256, step = 16)
    batch_size = trial.suggest_int('batch_size', low = 8, high = 16, step = 1)


    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, start_from_epoch=10, baseline = 50)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)

    # random forest classifier object
    scores = []
    best_scores = []
    for train, test in StratifiedKFold(n_splits=K, shuffle=True).split(X,z):
        model = create_model(num_ft_layers = num_ft_layers, neurons1 = neurons1, neurons2 = neurons2)
        try:
          history = model.fit([X[train], z[train]], y[train],
                                    validation_data = ([X[test], z[test]],y[test]), epochs=100, batch_size=batch_size, callbacks= [callback, reduce_lr]).history
          scores.append(model.evaluate([X[test], z[test]], y[test], verbose = 0))# evaluate the test dataset
          best_scores.append(min(history['val_loss']))
        except:
          traceback.print_exc()
          return(2000)
    the_best_score.append(np.mean(best_scores))
    return(np.mean(best_scores))


the_best_score = []
# create a study (aim to maximize score)
study = optuna.create_study(sampler=TPESampler(), direction='minimize')

# perform hyperparamter tuning (while timing the process)
time_start = time.time()
study.optimize(objective, n_trials=3, gc_after_trial=True)
time_bayesian = time.time() - time_start

columns = ['Number of iterations', 'Iteration Number of Optimal Hyperparamters', 'Score', 'Time Elapsed (s)']

# store result in a data frame
values_bayesian = [100, study.best_trial.number, study.best_trial.value, time_bayesian]
results_bayesian = pd.DataFrame([values_bayesian], columns = columns)

values = [study.trials[i].values[0] for i in range(len(study.trials))]
params = [study.trials[i].params for i in range(len(study.trials))]
columns = list(params[0].keys()) + ['val']
bayesian_results = pd.DataFrame(columns = columns)
bayesian_results['val'] = values
for param in list(params[0].keys()):
  param_list = [study[param] for study in params]
  bayesian_results[param] = param_list

bayesian_results.sort_values(by=['val'])

optuna.visualization.plot_param_importances(study)

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, start_from_epoch=10, baseline = 50)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)
model = create_model(8, 18, 18)
history = model.fit([dup_bae_3d_im_train, bae_3d_sexes_train], bae_3d_ages_train, validation_data= ([dup_bae_3d_im_test, bae_3d_sexes_test], bae_3d_ages_test), epochs = 100, batch_size = 12, callbacks = [callback, reduce_lr]).history

# Access the loss values from the training history
train_loss = history['loss']
val_loss = history['val_loss']

# Access the number of epochs
epochs = range(1, len(train_loss) + 1)

# Plot the training and validation curves
plt.plot(epochs, train_loss, 'b', label='Training Loss')
plt.plot(epochs, val_loss, 'r', label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

epochs = range(10, len(train_loss) + 1)

# Plot the training and validation curves
plt.plot(epochs, train_loss[9:], 'b', label='Training Loss')
plt.plot(epochs, val_loss[9:], 'r', label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

predictions = model.predict([dup_bae_3d_im_test, bae_3d_sexes_test])



predictions = predictions.reshape(-1)

color = {1:'orange',-1:'blue'}
label = {1:'Male',-1:'Female'}
#plt.scatter(predictions, y_test, c=[color[i] for i in sex_test], label='Data points', label = 'a')
plt.plot([60,90], [60,90], c='green', label='x=y')

for g in [-1,1]:
    i = np.where(bae_3d_sexes_test == g)
    plt.scatter(predictions[i], bae_3d_ages_test[i], c=color[g], label = label[g])

a, b = np.polyfit(predictions, bae_3d_ages_test, 1)


#add line of best fit to plot
plt.plot(predictions, a*predictions+b)


# Add labels and title
plt.xlabel('Predicted Age')
plt.ylabel('Actual Age')
plt.title('Scatter Plot of Predictions vs. Actual Age')

# Add a legend
plt.legend()

# Show the plot
plt.show()

adjusted_ages = lr_model.predict(predictions)

color = {1:'orange',-1:'blue'}
label = {1:'Male',-1:'Female'}
#plt.scatter(predictions, y_test, c=[color[i] for i in sex_test], label='Data points', label = 'a')
plt.plot([60,90], [60,90], c='green', label='x=y')

for g in [-1,1]:
    i = np.where(bae_3d_sexes_test == g)
    plt.scatter(np.array(adjusted_ages)[i], bae_3d_ages_test[i], c=color[g], label = label[g])

# Add labels and title
plt.xlabel('Predicted Age')
plt.ylabel('Actual Age')
plt.title('Scatter Plot of Predictions vs. Actual Age')

# Add a legend
plt.legend()

# Show the plot
plt.show()

age_gap = [predictions[i] - bae_3d_ages_test[i] for i in range(len(predictions))]
age_gap = np.array([i for i in age_gap])

from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from scipy.stats import pearsonr

def evaluate_predictions(predictions, true_values):
    # Calculate R^2
    r2 = r2_score(true_values, predictions)

    # Calculate MSE
    mse = mean_squared_error(true_values, predictions)

    # Calculate RMSE
    rmse = np.sqrt(mse)

    # Calculate MAE
    mae = mean_absolute_error(true_values, predictions)

    # Calculate Pearson's correlation coefficient
    r, _ = pearsonr(true_values, predictions)



    return r2, rmse, mse, mae, r

# Sample data
# (You'll need to define 'predictions' and 'bae_3d_ages_test' before calling the function)

r2, rmse, mse, mae, pearson_r = evaluate_predictions([float(i) for i in adjusted_ages], bae_3d_ages_test)
print(f"R^2: {r2}")
print(f"RMSE: {rmse}")
print(f"MSE: {mse}")
print(f"MAE: {mae}")
print(f"Pearson's r: {pearson_r}")

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, start_from_epoch=10, baseline = 50)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)
all_predictions = []
all_true_ages = []
for train, test in StratifiedKFold(n_splits=5, shuffle=True).split(dup_bae_3d_im_train,bae_3d_sexes_train):
  model = create_model(10, 18, 18)
  model.fit([dup_bae_3d_im_train[train], bae_3d_sexes_train[train]], bae_3d_ages_train[train],
            validation_data= ([dup_bae_3d_im_train[test], bae_3d_sexes_train[test]], bae_3d_ages_train[test]),
            epochs = 10, batch_size = 12, callbacks = [callback, reduce_lr])
  preds = model.predict([dup_bae_3d_im_train[test], bae_3d_sexes_train[test]])
  all_predictions = all_predictions + [i[0] for i in preds]
  all_true_ages = all_true_ages + [i for i in bae_3d_ages_train[test]]

#plt.scatter(predictions, y_test, c=[color[i] for i in sex_test], label='Data points', label = 'a')
plt.plot([60,90], [60,90], c='green', label='x=y')


plt.scatter(all_predictions, all_true_ages)

#find line of best fit
a, b = np.polyfit(all_predictions, all_true_ages, 1)


#add line of best fit to plot
plt.plot(all_predictions, (a*np.array(all_predictions))+b)

# Add labels and title
plt.xlabel('Predicted Age')
plt.ylabel('Actual Age')
plt.title('Scatter Plot of Predictions vs. Actual Age')

# Add a legend
plt.legend()

# Show the plot
plt.show()

all_age_gaps = [i - j for i,j in zip(all_predictions, all_true_ages)]


plt.axhline(y= 0, c='green', label='y=0')


plt.scatter(all_true_ages, all_age_gaps)

#find line of best fit
a, b = np.polyfit(all_true_ages, all_age_gaps, 1)

#add line of best fit to plot
plt.plot(all_true_ages, (a*np.array(all_true_ages))+b)

# Add labels and title
plt.xlabel('Actual Age')
plt.ylabel('Age gap')
plt.title('Scatter Plot of Predictions vs. Actual Age')

# Add a legend
plt.legend()

# Show the plot
plt.show()

r, _ = pearsonr(all_true_ages, all_age_gaps)
print(r)

from sklearn.linear_model import LinearRegression
import numpy as np

# Sample data
x = all_true_ages
y = all_predictions

# Reshape the data
x_reshaped = np.array(x).reshape(-1, 1)
y_reshaped = np.array(y)

# Fit the linear regression model
lr_model = LinearRegression().fit(x_reshaped, y_reshaped)

# Get the coefficients
a = lr_model.coef_[0]
b = lr_model.intercept_

print(f"a (slope) = {a}")
print(f"b (intercept) = {b}")

"""y = 0.27272814355506747x + 56.23879464552127
y is chronological age
x is predictions
"""

color = {1:'orange',-1:'blue'}
label = {1:'Male',-1:'Female'}
#plt.scatter(predictions, y_test, c=[color[i] for i in sex_test], label='Data points', label = 'a')
plt.axhline(y= 0, c='green', label='y=0')

for g in [-1,1]:
    i = np.where(bae_3d_sexes_test == g)
    plt.scatter(bae_3d_ages_test[i], age_gap[i], c=color[g], label = label[g])

#find line of best fit
a, b = np.polyfit(bae_3d_ages_test, age_gap, 1)
#add line of best fit to plot
r, _ = pearsonr(bae_3d_ages_test, age_gap)


plt.plot(bae_3d_ages_test, (a*bae_3d_ages_test)+b, label = f'r = {r}')

# Add labels and title
plt.xlabel('Actual Age')
plt.ylabel('Age gap')
plt.title('Scatter Plot of Predictions vs. Actual Age')

# Add a legend
plt.legend()

# Show the plot
plt.show()

adjusted_age_gaps = np.array([(y-b)/(a-x) for x,y in zip(bae_3d_ages_test, predictions)])

adjusted_age_gaps

color = {1:'orange',-1:'blue'}
label = {1:'Male',-1:'Female'}
#plt.scatter(predictions, y_test, c=[color[i] for i in sex_test], label='Data points', label = 'a')
plt.axhline(y= 0, c='green', label='y=0')

for g in [-1,1]:
    i = np.where(bae_3d_sexes_test == g)
    plt.scatter(bae_3d_ages_test[i], adjusted_age_gaps[i], c=color[g], label = label[g])

#find line of best fit
a, b = np.polyfit(bae_3d_ages_test, adjusted_age_gaps, 1)
#add line of best fit to plot
r, _ = pearsonr(bae_3d_ages_test, adjusted_age_gaps)


plt.plot(bae_3d_ages_test, (a*bae_3d_ages_test)+b, label = f'r = {r}')

# Add labels and title
plt.xlabel('Actual Age')
plt.ylabel('Age gap')
plt.title('Scatter Plot of Predictions vs. Actual Age')

# Add a legend
plt.legend()

# Show the plot
plt.show()

from sklearn.linear_model import LinearRegression
import numpy as np

# Sample data
x = bae_3d_ages_test,
y = age_gap

# Reshape the data
x_reshaped = np.array(x).reshape(-1, 1)
y_reshaped = np.array(y)

# Fit the linear regression model
lr_model = LinearRegression().fit(x_reshaped, y_reshaped)

# Get the coefficients
a = lr_model.coef_[0]
b = lr_model.intercept_

print(f"a (slope) = {a}")
print(f"b (intercept) = {b}")

lr_model.predict([[87]])

from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from scipy.stats import pearsonr

def evaluate_predictions(predictions, true_values):
    # Calculate R^2
    r2 = r2_score(true_values, predictions)

    # Calculate MSE
    mse = mean_squared_error(true_values, predictions)

    # Calculate RMSE
    rmse = np.sqrt(mse)

    # Calculate MAE
    mae = mean_absolute_error(true_values, predictions)

    # Calculate Pearson's correlation coefficient
    r, _ = pearsonr(true_values, predictions)



    return r2, rmse, mse, mae, r

# Sample data
# (You'll need to define 'predictions' and 'bae_3d_ages_test' before calling the function)

r2, rmse, mse, mae, pearson_r = evaluate_predictions([float(i) for i in predictions], bae_3d_ages_test)
print(f"R^2: {r2}")
print(f"RMSE: {rmse}")
print(f"MSE: {mse}")
print(f"MAE: {mae}")
print(f"Pearson's r: {pearson_r}")

"""### Version 5 BO"""

from classification_models_3D.tfkeras import Classifiers

vgg19, preprocess_input = Classifiers.get('vgg19')
pretrained_model = vgg19(input_shape=(79, 95, 68, 3), weights='imagenet')

import keras.backend as K
from tensorflow.keras.initializers import GlorotUniform
from keras.optimizers import Adam

def create_model(num_ft_layers, neurons1, neurons2):

  # Define the input layer for the 3D Convolutional network
  model = Sequential()

  for layer in pretrained_model.layers:
      model.add(layer)

  # Freeze the weights of the pre-trained layers so they are not updated during training
  for layer in model.layers:
      layer.trainable = False


  # Flatten the output of the pre-trained model
  x = Flatten()(model.output)

  # Add Dense layers after concatenation
  x = Dense(neurons1, activation='relu')(x)

  x = Dense(neurons2, activation='relu')(x)

  additional_input = Input(shape=(1,))
  concatenated = concatenate([x, additional_input])
  # Concatenate the flattened output with the new input

  output = Dense(1, activation='linear')(concatenated)  # Use 'linear' activation for regression tasks

  # Create the final model
  model = Model(inputs=[pretrained_model.input, additional_input], outputs=output)

  for layer in model.layers[22 - num_ft_layers:]:
            layer.trainable = True

  session = K.get_session()
  for layer in model.layers[22 - num_ft_layers:22]:
      if layer.get_weights():
        weights_shape = layer.get_weights()[0].shape
        bias_shape = layer.get_weights()[1].shape

        # Re-initialize weights and biases
        new_weights = GlorotUniform()(weights_shape).numpy()
        new_biases = GlorotUniform()(bias_shape).numpy()

        layer.set_weights([new_weights, new_biases])

  METRICS = [keras.metrics.MeanAbsoluteError(name='mae')]


  # Compile the model
  model.compile(loss='mean_squared_error', optimizer='adam', metrics=METRICS)

  return(model)

dup_bae_3d_im_train = np.repeat(bae_3d_im_train, 3, axis=-1)
dup_bae_3d_im_test = np.repeat(bae_3d_im_test, 3, axis=-1)

from keras.backend import dropout
import time
import optuna
from optuna.samplers import TPESampler
from sklearn.model_selection import cross_val_score
from keras.optimizers import Adam
import traceback

def objective(trial, X = dup_bae_3d_im_train, y = bae_3d_ages_train, z = bae_3d_sexes_train,K = 3):
    global the_best_score
    """return the mse"""
    num_ft_layers = trial.suggest_int('num_layers', low = 0, high = 12, step = 2)
    neurons1 = trial.suggest_int('neurons1', low = 2, high = 256, step = 16)
    neurons2 = trial.suggest_int('neurons2', low = 2, high = 256, step = 16)
    batch_size = trial.suggest_int('batch_size', low = 8, high = 16, step = 1)


    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, start_from_epoch=10, baseline = 50)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)

    # random forest classifier object
    scores = []
    best_scores = []
    for train, test in StratifiedKFold(n_splits=K, shuffle=True).split(X,z):
        model = create_model(num_ft_layers = num_ft_layers, neurons1 = neurons1, neurons2 = neurons2)
        try:
          history = model.fit([X[train], z[train]], y[train],
                                    validation_data = ([X[test], z[test]],y[test]), epochs=100, batch_size=batch_size, callbacks= [callback, reduce_lr]).history
          scores.append(model.evaluate([X[test], z[test]], y[test], verbose = 0))# evaluate the test dataset
          best_scores.append(min(history['val_loss']))
        except:
          traceback.print_exc()
          return(2000)
    the_best_score.append(np.mean(best_scores))
    return(np.mean(best_scores))


the_best_score = []
# create a study (aim to maximize score)
study = optuna.create_study(sampler=TPESampler(), direction='minimize')

# perform hyperparamter tuning (while timing the process)
time_start = time.time()
study.optimize(objective, n_trials=3, gc_after_trial=True)
time_bayesian = time.time() - time_start

columns = ['Number of iterations', 'Iteration Number of Optimal Hyperparamters', 'Score', 'Time Elapsed (s)']

# store result in a data frame
values_bayesian = [100, study.best_trial.number, study.best_trial.value, time_bayesian]
results_bayesian = pd.DataFrame([values_bayesian], columns = columns)

values = [study.trials[i].values[0] for i in range(len(study.trials))]
params = [study.trials[i].params for i in range(len(study.trials))]
columns = list(params[0].keys()) + ['val']
bayesian_results = pd.DataFrame(columns = columns)
bayesian_results['val'] = values
for param in list(params[0].keys()):
  param_list = [study[param] for study in params]
  bayesian_results[param] = param_list

bayesian_results.sort_values(by=['val'])

optuna.visualization.plot_param_importances(study)

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, start_from_epoch=10, baseline = 50)
from keras.callbacks import ReduceLROnPlateau
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)
model = create_model(12, 82, 226)
history = model.fit([dup_bae_3d_im_train, bae_3d_sexes_train], bae_3d_ages_train, validation_data= ([dup_bae_3d_im_test, bae_3d_sexes_test], bae_3d_ages_test), epochs = 100, batch_size = 9, callbacks = [callback, reduce_lr]).history

# Access the loss values from the training history
train_loss = history['loss']
val_loss = history['val_loss']

# Access the number of epochs
epochs = range(1, len(train_loss) + 1)

# Plot the training and validation curves
plt.plot(epochs, train_loss, 'b', label='Training Loss')
plt.plot(epochs, val_loss, 'r', label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

epochs = range(10, len(train_loss) + 1)

# Plot the training and validation curves
plt.plot(epochs, train_loss[9:], 'b', label='Training Loss')
plt.plot(epochs, val_loss[9:], 'r', label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

predictions = model.predict([dup_bae_3d_im_test, bae_3d_sexes_test])

color = {1:'orange',-1:'blue'}
label = {1:'Male',-1:'Female'}
#plt.scatter(predictions, y_test, c=[color[i] for i in sex_test], label='Data points', label = 'a')
plt.plot([60,90], [60,90], c='green', label='x=y')

for g in [-1,1]:
    i = np.where(bae_3d_sexes_test == g)
    plt.scatter(predictions[i], bae_3d_ages_test[i], c=color[g], label = label[g])

# Add labels and title
plt.xlabel('Predicted Age')
plt.ylabel('Actual Age')
plt.title('Scatter Plot of Predictions vs. Actual Age')

# Add a legend
plt.legend()

# Show the plot
plt.show()

import numpy as np
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, confusion_matrix
from scipy.stats import pearsonr

def evaluate_predictions(predictions, true_values):
    # Calculate R^2
    r2 = r2_score(true_values, predictions)

    # Calculate MSE
    mse = mean_squared_error(true_values, predictions)

    # Calculate RMSE
    rmse = np.sqrt(mse)

    # Calculate MAE
    mae = mean_absolute_error(true_values, predictions)

    # Calculate Pearson's correlation coefficient
    r, _ = pearsonr(true_values, predictions)



    return r2, rmse, mse, mae, r

# Sample data
# (You'll need to define 'predictions' and 'bae_3d_ages_test' before calling the function)

r2, rmse, mse, mae, pearson_r = evaluate_predictions([float(i) for i in predictions], bae_3d_ages_test)
print(f"R^2: {r2}")
print(f"RMSE: {rmse}")
print(f"MSE: {mse}")
print(f"MAE: {mae}")
print(f"Pearson's r: {pearson_r}")

"""### V6"""

from classification_models_3D.tfkeras import Classifiers

vgg19, preprocess_input = Classifiers.get('vgg19')
pretrained_model = vgg19(input_shape=(79, 95, 68, 3), weights='imagenet')

duplicated_data = np.repeat(image_data, 3, axis=-1)
print(duplicated_data.shape)

from keras.backend import dropout
import time
import optuna
from optuna.samplers import TPESampler
from sklearn.model_selection import cross_val_score
from keras.optimizers import Adam
import traceback
import gc

def create_model(learning_rate, neurons1, neurons2):

  # Define the input layer for the 3D Convolutional network
  model = Sequential()

  for layer in pretrained_model.layers:
      model.add(layer)

  # Freeze the weights of the pre-trained layers so they are not updated during training
  for layer in model.layers:
      layer.trainable = False


  # Flatten the output of the pre-trained model
  x = Flatten()(model.output)

  # Add Dense layers after concatenation
  x = Dense(neurons1, activation='relu')(x)
  x = Dense(neurons2, activation='relu')(x)

  additional_input = Input(shape=(1,))
  # Concatenate the flattened output with the new input
  concatenated = concatenate([x, additional_input])
  output = Dense(1, activation='linear')(concatenated)  # Use 'linear' activation for regression tasks

  # Create the final model
  model = Model(inputs=[pretrained_model.input, additional_input], outputs=output)

  for layer in model.layers[19:]:
            layer.trainable = True


  METRICS = [keras.metrics.MeanSquaredError(name='mse'),
               keras.metrics.MeanAbsoluteError(name='mae')]
  optimizer = Adam(learning_rate=learning_rate)
  # Compile the model
  model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=METRICS)

  return(model)




def objective(trial, X = duplicated_data,y=ages,z=sexes,K = 5):
    global the_best_score
    """return the mse"""
    neurons1 = trial.suggest_int('neurons1', low = 2, high = 256, step = 16)
    neurons2 = trial.suggest_int('neurons2', low = 2, high = 256, step = 16)
    batch_size = trial.suggest_int('batch_size', low = 16, high = 64, step = 8)
    learning_rate = trial.suggest_float('learning_rate', low = 0.00001, high = 0.01, step = 0.0005)

    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=25, start_from_epoch=50, baseline = 100)
    # random forest classifier object
    scores = []
    best_scores = []
    for train, test in StratifiedKFold(n_splits=K, shuffle=True).split(X,z):
        model = create_model(neurons1 = neurons1, neurons2 = neurons2, learning_rate = learning_rate)
        try:
          history = model.fit([X[train], z[train]], y[train],
                                    validation_data = ([X[test], z[test]],y[test]), epochs=125, batch_size=batch_size, callbacks= [callback]).history
          scores.append(model.evaluate([X[test], z[test]], y[test], verbose = 0))# evaluate the test dataset
          best_scores.append(min(history['val_loss']))
        except:
          traceback.print_exc()
          return(2000)
    the_best_score.append(np.mean(best_scores))
    return(np.mean(scores))


the_best_score = []
# create a study (aim to maximize score)
study = optuna.create_study(sampler=TPESampler(), direction='minimize')

# perform hyperparamter tuning (while timing the process)
time_start = time.time()
study.optimize(objective, n_trials=20, gc_after_trial=True)
time_bayesian = time.time() - time_start

columns = ['Number of iterations', 'Iteration Number of Optimal Hyperparamters', 'Score', 'Time Elapsed (s)']

# store result in a data frame
values_bayesian = [100, study.best_trial.number, study.best_trial.value, time_bayesian]
results_bayesian = pd.DataFrame([values_bayesian], columns = columns)

"""### Version 7 from paper"""

import numpy as np
from keras.models import Sequential
from keras.layers import Conv3D, MaxPooling3D, BatchNormalization, Dense, Flatten, Activation
from keras.optimizers import SGD

def create_model():
    model = Sequential()

    # Number of feature channels
    num_channels = 8

    # Repeated 5 blocks
    for _ in range(4):
        # (3x3x3) Convolutional layer with stride of 1
        model.add(Conv3D(filters=num_channels, kernel_size=(3, 3, 3), strides=(1, 1, 1), padding='same', input_shape=(79, 95, 68, 1)))
        model.add(Activation('relu'))

        # Another (3x3x3) Convolutional layer with stride of 1
        model.add(Conv3D(filters=num_channels, kernel_size=(3, 3, 3), strides=(1, 1, 1), padding='same'))

        # 3D Batch-normalization layer
        model.add(BatchNormalization())

        # ReLU
        model.add(Activation('relu'))

        # (2x2x2) Max-pooling layer with stride of 2
        model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2)))

        # Double the number of feature channels for the next block
        num_channels *= 2

    # Flatten the output
    model.add(Flatten())

    # Fully connected layer for age prediction
    model.add(Dense(1))
    sgd = SGD(learning_rate=0.01, momentum=0.9)

    model.compile(optimizer=sgd, loss='mean_absolute_error')
    return model

# Assuming you have your data for predictions, test_labels, and sex_train
# Create a scatter plot
callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, start_from_epoch=100, baseline = 50)
from keras.callbacks import ReduceLROnPlateau
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)

model = create_model()
history = model.fit(bae_3d_im_train, bae_3d_ages_train, validation_data = (bae_3d_im_test, bae_3d_ages_test), epochs = 200, batch_size = 28, callbacks = [callback, reduce_lr]).history
predictions = model.predict(bae_3d_im_test)

# Access the loss values from the training history
train_loss = history['loss']
val_loss = history['val_loss']

# Access the number of epochs
epochs = range(1, len(train_loss) + 1)

# Plot the training and validation curves
plt.plot(epochs, train_loss, 'b', label='Training Loss')
plt.plot(epochs, val_loss, 'r', label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

epochs = range(25, len(train_loss) + 1)

# Plot the training and validation curves
plt.plot(epochs, train_loss[24:], 'b', label='Training Loss')
plt.plot(epochs, val_loss[24:], 'r', label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

epochs = range(100, len(train_loss) + 1)

# Plot the training and validation curves
plt.plot(epochs, train_loss[99:], 'b', label='Training Loss')
plt.plot(epochs, val_loss[99:], 'r', label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

predictions = model.predict(bae_3d_im_test)

color = {1:'orange',-1:'blue'}
label = {1:'Male',-1:'Female'}
#plt.scatter(predictions, y_test, c=[color[i] for i in sex_test], label='Data points', label = 'a')
plt.plot([60,90], [60,90], c='green', label='x=y')

for g in [-1,1]:
    i = np.where(bae_3d_sexes_test == g)
    plt.scatter(predictions[i], bae_3d_ages_test[i], c=color[g], label = label[g])

# Add labels and title
plt.xlabel('Predicted Age')
plt.ylabel('Actual Age')
plt.title('Scatter Plot of Predictions vs. Actual Age')

# Add a legend
plt.legend()

# Show the plot
plt.show()

import numpy as np
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, confusion_matrix
from scipy.stats import pearsonr

def evaluate_predictions(predictions, true_values):
    # Calculate R^2
    r2 = r2_score(true_values, predictions)

    # Calculate MSE
    mse = mean_squared_error(true_values, predictions)

    # Calculate RMSE
    rmse = np.sqrt(mse)

    # Calculate MAE
    mae = mean_absolute_error(true_values, predictions)

    # Calculate Pearson's correlation coefficient
    r, _ = pearsonr(true_values, predictions)



    return r2, rmse, mse, mae, r

# Sample data
# (You'll need to define 'predictions' and 'bae_3d_ages_test' before calling the function)

r2, rmse, mse, mae, pearson_r = evaluate_predictions([float(i) for i in predictions], bae_3d_ages_test)
print(f"R^2: {r2}")
print(f"RMSE: {rmse}")
print(f"MSE: {mse}")
print(f"MAE: {mae}")
print(f"Pearson's r: {pearson_r}")



"""### Extended data model"""

dup_full_3d_im_train = np.repeat(full_3d_im_train, 3, axis=-1)
dup_full_3d_im_test = np.repeat(full_3d_im_test, 3, axis=-1)

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, start_from_epoch=10, baseline = 50)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)
model = create_model(8, 18, 18)
history = model.fit([dup_full_3d_im_train, full_3d_sexes_train], full_3d_ages_train,
                    validation_data= ([dup_full_3d_im_test, full_3d_sexes_test], full_3d_ages_test), epochs = 100,
                    batch_size = 16, callbacks = [callback, reduce_lr]).history

# Access the loss values from the training history
train_loss = history['loss']
val_loss = history['val_loss']

# Access the number of epochs
epochs = range(1, len(train_loss) + 1)

# Plot the training and validation curves
plt.plot(epochs, train_loss, 'b', label='Training Loss')
plt.plot(epochs, val_loss, 'r', label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

epochs = range(25, len(train_loss) + 1)

# Plot the training and validation curves
plt.plot(epochs, train_loss[24:], 'b', label='Training Loss')
plt.plot(epochs, val_loss[24:], 'r', label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

len(predictions)

predictions = []
for i in range(84):
  prediction =  model.predict([dup_full_3d_im_test[i:i+1], full_3d_sexes_test[i:i+1]])
  predictions.append(prediction)

predictions = [i[0][0] for i in predictions]

full_3d_sexes_test

color = {1:'orange',-1:'blue'}
label = {1:'Male',-1:'Female'}
#plt.scatter(predictions, y_test, c=[color[i] for i in sex_test], label='Data points', label = 'a')
plt.plot([60,90], [60,90], c='green', label='x=y')

for g in [-1,1]:
    i = np.where(full_3d_sexes_test == g)
    plt.scatter(np.array(predictions)[i], full_3d_ages_test[i], c=color[g], label = label[g])

# Add labels and title
plt.xlabel('Predicted Age')
plt.ylabel('Actual Age')
plt.title('Scatter Plot of Predictions vs. Actual Age')

# Add a legend
plt.legend()

# Show the plot
plt.show()

import numpy as np
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, confusion_matrix
from scipy.stats import pearsonr

def evaluate_predictions(predictions, true_values):
    # Calculate R^2
    r2 = r2_score(true_values, predictions)

    # Calculate MSE
    mse = mean_squared_error(true_values, predictions)

    # Calculate RMSE
    rmse = np.sqrt(mse)

    # Calculate MAE
    mae = mean_absolute_error(true_values, predictions)

    # Calculate Pearson's correlation coefficient
    r, _ = pearsonr(true_values, predictions)



    return r2, rmse, mse, mae, r

# Sample data
# (You'll need to define 'predictions' and 'bae_3d_ages_test' before calling the function)

r2, rmse, mse, mae, pearson_r = evaluate_predictions(predictions, full_3d_ages_test)
print(f"R^2: {r2}")
print(f"RMSE: {rmse}")
print(f"MSE: {mse}")
print(f"MAE: {mae}")
print(f"Pearson's r: {pearson_r}")

"""### 3D saliency map"""

import tensorflow as tf

def compute_saliency_3d(model, image_input, extra_input):
    # Create a TensorFlow Variable from the image input
    image_input_tensor = tf.Variable(image_input, dtype=tf.float32)

    # Compute the gradient
    with tf.GradientTape() as tape:
        tape.watch(image_input_tensor)
        predictions = model([image_input_tensor, extra_input])
        loss = tf.reduce_sum(predictions)  # Summing all predictions to get a scalar value

    gradients = tape.gradient(loss, image_input_tensor)
    gradients = tf.reduce_max(gradients, axis=-1)
    gradients = gradients.numpy()

    min_val, max_val = np.min(gradients), np.max(gradients)
    saliency_map = (gradients - min_val) / (max_val - min_val + keras.backend.epsilon())


    # Compute the saliency map
    #saliency_map = tf.math.abs(gradients).numpy()

    return saliency_map



saliency = compute_saliency_3d(model, dup_bae_3d_im_test[:10], bae_3d_sexes_test[:10])

saliency.shape

import matplotlib.pyplot as plt

def plot_saliency_3d_slices(data, slice_index, vmin, vmax):
    slice_dim = 'sagittal'
    slice_data = np.rot90(data[:, :, slice_index])
    plt.imshow(slice_data, cmap="viridis", vmin=vmin, vmax=vmax)
    plt.colorbar()
    plt.title(f'{slice_dim.capitalize()} Saliency Slice at Index {slice_index}')
    plt.show()

def plot_3d_slices(data, slice_index):
    slice_dim = 'sagittal'
    slice_data = np.rot90(data[:, :, slice_index])
    plt.imshow(slice_data, cmap="bone")
    plt.title(f'{slice_dim.capitalize()} Slice at Index {slice_index}')
    plt.show()

vmin = np.min([im for i,im in enumerate(saliency[i]) if i in [5,15,25,35,45,55,65]])
vmax = np.max([im for i,im in enumerate(saliency[i]) if i in [5,15,25,35,45,55,65]])
i = 5
for idx in [5,15,25,35,45,55,65]:
  # Visualize a sagittal slice
  plot_saliency_3d_slices(saliency[i], idx, vmin = vmin, vmax=vmax)
  plot_3d_slices(dup_bae_3d_im_test[i], idx)

import numpy as np
import matplotlib.pyplot as plt
i = 5

def plot_saliency_3d_slices(ax, data, slice_index, vmin, vmax):
    slice_dim = 'sagittal'
    slice_data = np.rot90(data[:, :, slice_index])
    im = ax.imshow(slice_data, cmap="viridis", vmin=vmin, vmax=vmax)
    ax.set_title(f'{slice_dim.capitalize()} Saliency Slice at Index {slice_index}')
    return im

def plot_3d_slices(ax, data, slice_index):
    slice_dim = 'sagittal'
    slice_data = np.rot90(data[:, :, slice_index])
    ax.imshow(slice_data, cmap="bone")
    ax.set_title(f'{slice_dim.capitalize()} Slice at Index {slice_index}')

# Determine global min and max values for the color scale
vmin = np.min(saliency[i])
vmax = np.max(saliency[i])

# Create a 4x4 grid of subplots
fig, axes = plt.subplots(4, 4, figsize=(18, 20))

indices = [5, 15, 25, 35, 45, 55, 65]
for j, idx in enumerate(indices):
    ax1 = axes[j // 2, j % 2 * 2]  # Determine the row and column in the grid for saliency
    ax2 = axes[j // 2, j % 2 * 2 + 1]  # Determine the row and column in the grid for original image

    im = plot_saliency_3d_slices(ax1, saliency[i], idx, vmin, vmax)
    plot_3d_slices(ax2, dup_bae_3d_im_test[i], idx)

# Hide the axes for the unused subplots
for k in range(6, 16):
    axes[k // 4, k % 4].axis('off')

# Adjust the layout to make space for the colorbar
fig.subplots_adjust(right=0.85)

# Add a single colorbar for the entire figure
cbar_ax = fig.add_axes([0.88, 0.15, 0.02, 0.7])
fig.colorbar(im, cax=cbar_ax)

plt.show()

def plot_grid_saliency_3d_slices(ax, data, slice_index):
    slice_dim = 'sagittal'
    slice_data = np.rot90(data[:, :, slice_index])
    ax.imshow(slice_data, cmap="viridis")
    ax.set_title(f'{slice_dim.capitalize()} Slice at Index {slice_index}')

# Create a 4x5 grid of subplots
fig, axes = plt.subplots(4, 5, figsize=(20, 16))

for j in range(17):
    idx = j * 4
    ax = axes[j // 5, j % 5]  # Determine the row and column in the grid
    plot_grid_saliency_3d_slices(ax, averaged_map[0], idx)

# Hide the axes for the last 3 subplots
for i in range(3):
    axes[3, 4-i].axis('off')

plt.tight_layout()
plt.show()

def plot_grid_saliency_3d_slices(ax, data, slice_index, vmin, vmax):
    slice_dim = 'sagittal'
    slice_data = np.rot90(data[:, :, slice_index])
    im = ax.imshow(slice_data, cmap="viridis", vmin=vmin, vmax=vmax)
    ax.set_title(f'{slice_dim.capitalize()} Slice at Index {slice_index}')
    return im

# Determine global min and max values for the color scale
vmin = np.min(averaged_map[0])
vmax = np.max(averaged_map[0])

# Create a 5x4 grid of subplots
fig, axes = plt.subplots(5, 4, figsize=(18, 20))

for j in range(17):
    idx = j * 4
    ax = axes[j // 4, j % 4]  # Determine the row and column in the grid
    im = plot_grid_saliency_3d_slices(ax, averaged_map[0], idx, vmin, vmax)

# Hide the axes for the last 3 subplots
for i in range(3):
    axes[4, 3-i].axis('off')

# Adjust the layout to make space for the colorbar
fig.subplots_adjust(right=0.85)

# Add a single colorbar for the entire figure
cbar_ax = fig.add_axes([0.88, 0.15, 0.02, 0.7])
fig.colorbar(im, cax=cbar_ax)

plt.show()

import numpy as np

# Assuming saliency_maps is your array of shape (10, 79, 95, 68, 3)

# Average across the 10 images
averaged_map = np.mean(saliency, axis=0)

# To make it of shape (1, 79, 95, 68, 3)
averaged_map = averaged_map[np.newaxis, ...]

for j in range(17):
  idx = j * 4
  # Visualize a sagittal slice
  plot_saliency_3d_slices(averaged_map[0], idx)

"""## 2D

### Creating the 2D data
"""

from skimage.feature import greycomatrix
from skimage import img_as_ubyte
twod_df = pd.DataFrame(columns = ['id','age','sex','entropy','data', 'split'])
slice_counter = 0
rotation_angle = 90
counter = 0
for image_array, age, sex in zip(image_data, ages, sexes):
  counter += 1
  nx, ny, nz, nw = image_array.shape
  total_volumes = image_array.shape[3]
  total_slices = image_array.shape[2]
  if counter -1 in train_idx:
    split = 'train'
  else:
    split = 'test'
  for current_volume in range(0, total_volumes):
      # iterate through slices
      for current_slice in range(0, total_slices):
          if (slice_counter % 1) == 0:
              if rotation_angle == 90:
                  data = np.rot90(image_array[:, :, current_slice, current_volume])
              elif rotation_angle == 180:
                  data = np.rot90(np.rot90(image_array[:, :, current_slice, current_volume]))
              elif rotation_angle == 270:
                  data = np.rot90(np.rot90(np.rot90(image_array[:, :, current_slice, current_volume])))
              #alternate slices and save as png
              if (slice_counter % 1) == 0:
                  print('Adding image...')
                  min_val = np.min(data)
                  max_val = np.max(data)
                  im = 2 * (data - min_val) / (max_val - min_val) - 1
                  im = img_as_ubyte(im)
                  glcm = np.squeeze(greycomatrix(im, distances=[1], angles=[0], symmetric=True, normed=True))
                  entropy = -np.sum(glcm*np.log2(glcm + (glcm==0)))

                  new_row = {'id':counter, 'age':age, 'sex':sex, 'entropy':entropy, 'data':data, 'split':split}
                  twod_df = twod_df.append(new_row, ignore_index=True)
                  #imsave(imge_folder +str(counter)+'_'+x_label2+'.jpg', data,format='jpg')
                  print('Saved.')

plt.imshow(twod_df['data'][twod_df['entropy'].idxmax()], cmap='bone')
plt.axis('off')
plt.show()
# Syntax

plt.imshow(twod_df['data'][twod_df['entropy'].idxmin()], cmap='bone')
plt.axis('off')
plt.show()
# Syntax

high_entropy_df = pd.DataFrame(columns = ['id','age','sex','entropy','data', 'split'])
for id in twod_df.id.unique():
  filtered_df = twod_df[twod_df.id == id].sort_values(by=['entropy'], ascending=False).head(50)
  high_entropy_df = high_entropy_df.append(filtered_df)

high_entropy_df

group_id = np.array(high_entropy_df[high_entropy_df.split == 'train']['id'])

bae_2d_X_train = np.array(high_entropy_df[high_entropy_df.split == 'train']['data'])
bae_2d_y_train = np.array(high_entropy_df[high_entropy_df.split == 'train']['age'])
bae_2d_X_test = np.array(high_entropy_df[high_entropy_df.split == 'test']['data'])
bae_2d_y_test = np.array(high_entropy_df[high_entropy_df.split == 'test']['age'])

bae_2d_X_train = np.array([bae_2d_X_train[i] for i in range(len(bae_2d_X_train))])
bae_2d_y_train = np.array([bae_2d_y_train[i] for i in range(len(bae_2d_y_train))])
bae_2d_X_test = np.array([bae_2d_X_test[i] for i in range(len(bae_2d_X_test))])
bae_2d_y_test = np.array([bae_2d_y_test[i] for i in range(len(bae_2d_y_test))])

bae_2d_z_train =  np.array(high_entropy_df[high_entropy_df.split == 'train']['sex'])
bae_2d_z_test =  np.array(high_entropy_df[high_entropy_df.split == 'test']['sex'])
bae_2d_z_train = np.array([bae_2d_z_train[i] for i in range(len(bae_2d_z_train))])
bae_2d_z_test = np.array([bae_2d_z_test[i] for i in range(len(bae_2d_z_test))])

"""### V1"""

def compile_model():
  model = Sequential() # using sequential

  # Add 3D Convolutional layers with MaxPooling
  model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(95, 79, 1)))
  model.add(MaxPooling2D(pool_size=(2, 2)))
  model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(95, 79, 1)))
  model.add(MaxPooling2D(pool_size=(2, 2)))


  # Flatten the output and add Dense layers for age prediction
  model.add(Flatten())
  model.add(Dense(128, activation='relu'))
  model.add(Dense(1, activation='linear'))  # Use 'linear' activation for regression tasks

  # Compile the model
  model.compile(loss='mean_squared_error', optimizer='adam', metrics=[metrics.mean_absolute_error])

  return(model)

def cross_validate(X, y, K = 5):
    scores = []
    histories = []
    r2 = []
    for train, test in KFold(n_splits=K, shuffle=True).split(range(102)):
        model = compile_model() # compile model
        train = [i for i,a in enumerate(group_id) if a in train+1]
        test = [i for i,a in enumerate(group_id) if a in test+1]
        histories.append(model.fit(X[train], y[train],
                                   validation_data = (X[test],y[test]), epochs=200, batch_size=16 # feed in the test data for plotting
                                   ).history)
        scores.append(model.evaluate(X[test], y[test], verbose = 0)) # evaluate the test dataset
        r2.append(r2_score(model.predict(X[test]), y[test]))
    print("average test MSE: ", np.asarray(scores)[:,0].mean())
    print("average test MAE: ", np.asarray(scores)[:,1].mean())
    print("average test r2: ", np.mean(r2))
    print(model.summary())
    return scores, histories, r2

scores, histories, r2 = cross_validate(X, y, K = 5)

model = compile_model()

# Assuming you have your data for predictions, test_labels, and sex_train
# Create a scatter plot
model = compile_model()
for train, test in KFold(n_splits=5, shuffle=True).split(range(102)):
    train = [i for i,a in enumerate(group_id) if a in train+1]
    test = [i for i,a in enumerate(group_id) if a in test+1]
    break
history = model.fit(X[train], y[train], validation_data = (X[test],y[test]), epochs=200, batch_size=16).history
predictions = model.predict(X[test])

color = {1:'orange',-1:'blue'}
label = {1:'Male',-1:'Female'}
#plt.scatter(predictions, y_test, c=[color[i] for i in sex_test], label='Data points', label = 'a')
plt.plot([60,90], [60,90], c='green', label='x=y')

for g in [-1,1]:
    i = np.where(z[test] == g)
    print()
    plt.scatter(predictions[i], y[test][i], c=color[g], label = label[g])

# Add labels and title
plt.xlabel('Predicted Age')
plt.ylabel('Actual Age')
plt.title('Scatter Plot of Predictions vs. Actual Age')

# Add a legend
plt.legend()

# Show the plot
plt.show()

# Access the loss values from the training history
train_loss = history['loss']
val_loss = history['val_loss']

# Access the number of epochs
epochs = range(1, len(train_loss) + 1)

# Plot the training and validation curves
plt.plot(epochs, train_loss, 'b', label='Training Loss')
plt.plot(epochs, val_loss, 'r', label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

epochs = range(10, len(train_loss) + 1)

# Plot the training and validation curves
plt.plot(epochs, train_loss[9:], 'b', label='Training Loss')
plt.plot(epochs, val_loss[9:], 'r', label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""### V2"""

from sklearn import metrics
from sklearn.metrics import r2_score
from keras import metrics

def compile_model():

  # Define the input layer for the 3D Convolutional network
  input_img = Input(shape=(95, 79, 1))

  # Apply 3D Convolutional layers with MaxPooling using the Functional API
  x = Conv2D(32, kernel_size=(3, 3), activation='relu')(input_img)
  x = MaxPooling2D(pool_size=(2, 2))(x)
  x = Conv2D(64, kernel_size=(3, 3), activation='relu')(x)
  x = MaxPooling2D(pool_size=(2, 2))(x)


  # Flatten the output and add Dense layers for age prediction
  x = Flatten()(x)
  x = Dense(128, activation='relu')(x)
  output_img = Dense(32, activation='linear')(x)

  # Define the input layer for the extra feature
  input_extra = Input(shape=(1,))

  # Concatenate the output of the Convolutional network with the extra feature
  merged = concatenate([output_img, input_extra])

  # Add Dense layers for further prediction
  merged = Dense(32, activation='relu')(merged)
  merged = Dense(4, activation='relu')(merged)
  output = Dense(1, activation='linear')(merged)  # Use 'linear' activation for regression tasks

  # Define the model with both inputs and the final output
  model = Model(inputs=[input_img, input_extra], outputs=output)

  # Compile the model
  model.compile(loss='mean_squared_error', optimizer='adam', metrics=[metrics.mean_absolute_error])

  return(model)

def cross_validate(X, y, z, K = 5):

    scores = []
    histories = []
    r2 = []
    for train, test in KFold(n_splits=K, shuffle=True).split(range(102)):
        model = compile_model() # compile model
        train = [i for i,a in enumerate(group_id) if a in train+1]
        test = [i for i,a in enumerate(group_id) if a in test+1]
        histories.append(model.fit([X[train], z[train]], y[train],
                                   validation_data = ([X[test], z[test]],y[test]), epochs=200, batch_size=16 # feed in the test data for plotting
                                   ).history)
        scores.append(model.evaluate([X[test], z[test]], y[test], verbose = 0)) # evaluate the test dataset
        r2.append(r2_score(model.predict([X[test], z[test]]), y[test]))
    print("average test MSE: ", np.asarray(scores)[:,0].mean())
    print("average test MAE: ", np.asarray(scores)[:,1].mean())
    print("average test r2: ", np.mean(r2))
    print(model.summary())
    return scores, histories, r2

scores_v2, histories_v2, r2_v2 = cross_validate(X, y, z, K = 5)

scores_v2

np.mean([4.432161808013916,
4.530690670013428,
4.64683198928833,
4.610693454742432])

np.mean([28.079561233520508,
33.709259033203125,
30.072385787963867,
31.423490524291992])

np.mean([-0.9116567749839588,
 -2.4629745336738234,
 -0.4520105970184283,
 -1.8885788782523218])

model = compile_model()

# Assuming you have your data for predictions, test_labels, and sex_train
# Create a scatter plot
model = compile_model()
for train, test in KFold(n_splits=6, shuffle=True).split(range(102)):
    train = [i for i,a in enumerate(group_id) if a in train+1]
    test = [i for i,a in enumerate(group_id) if a in test+1]
    break
history = model.fit([X[train],z[train]], y[train], validation_data = ([X[test], z[test]],y[test]), epochs=200, batch_size=16).history
predictions = model.predict([X[test], z[test]])

color = {1:'orange',-1:'blue'}
label = {1:'Male',-1:'Female'}
#plt.scatter(predictions, y_test, c=[color[i] for i in sex_test], label='Data points', label = 'a')
plt.plot([60,90], [60,90], c='green', label='x=y')

for g in [-1,1]:
    i = np.where(z[test] == g)
    print()
    plt.scatter(predictions[i], y[test][i], c=color[g], label = label[g])

# Add labels and title
plt.xlabel('Predicted Age')
plt.ylabel('Actual Age')
plt.title('Scatter Plot of Predictions vs. Actual Age')

# Add a legend
plt.legend()

# Show the plot
plt.show()

# Access the loss values from the training history
train_loss = history['loss']
val_loss = history['val_loss']

# Access the number of epochs
epochs = range(1, len(train_loss) + 1)

# Plot the training and validation curves
plt.plot(epochs, train_loss, 'b', label='Training Loss')
plt.plot(epochs, val_loss, 'r', label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

epochs = range(10, len(train_loss) + 1)

# Plot the training and validation curves
plt.plot(epochs, train_loss[9:], 'b', label='Training Loss')
plt.plot(epochs, val_loss[9:], 'r', label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""### Version 1"""

from keras.applications.vgg19 import VGG19
pretrained_model = VGG19(input_shape=(95, 79, 3), include_top=False)

bae_2d_X_train_vgg = np.repeat(bae_2d_X_train[:, :, :, np.newaxis], 3, axis=3)
bae_2d_X_test_vgg = np.repeat(bae_2d_X_test[:, :, :, np.newaxis], 3, axis=3)

def create_model(num_ft_layers,neurons1, neurons2):

  # Define the input layer for the 3D Convolutional network
  model = Sequential()

  for layer in pretrained_model.layers[:14]:
      model.add(layer)

  # Freeze the weights of the pre-trained layers so they are not updated during training
  for layer in model.layers:
      layer.trainable = False


  # Flatten the output of the pre-trained model
  x = Flatten()(model.output)

  additional_input = Input(shape=(1,))


  # Add Dense layers after concatenation
  x = Dense(neurons1, activation='relu')(x)
  x = Dense(neurons2, activation='relu')(x)
  x = concatenate([x, additional_input])
  # Concatenate the flattened output with the new input
  output = Dense(1, activation='linear')(x)  # Use 'linear' activation for regression tasks

  # Create the final model
  model = Model(inputs=[pretrained_model.input, additional_input], outputs=output)

  for layer in model.layers[14 - num_ft_layers:]:
            layer.trainable = True

  session = K.get_session()
  for layer in model.layers[14 - num_ft_layers:14]:
      if layer.get_weights():
        weights_shape = layer.get_weights()[0].shape
        bias_shape = layer.get_weights()[1].shape

        # Re-initialize weights and biases
        new_weights = GlorotUniform()(weights_shape).numpy()
        new_biases = GlorotUniform()(bias_shape).numpy()

        layer.set_weights([new_weights, new_biases])
  METRICS = [keras.metrics.MeanAbsoluteError(name='mae')]



  # Compile the model
  model.compile(loss='mean_squared_error', optimizer='adam', metrics=METRICS)

  return(model)

import random

def objective(trial, X = bae_2d_X_train_vgg,y = bae_2d_y_train,z=bae_2d_z_train ,K = 3):
    global the_best_score
    """return the mse"""
    num_ft_layers = trial.suggest_int('num_layers', low = 0, high = 12, step = 2)
    neurons1 = trial.suggest_int('neurons1', low = 2, high = 256, step = 16)
    neurons2 = trial.suggest_int('neurons2', low = 2, high = 256, step = 16)
    batch_size = trial.suggest_int('batch_size', low = 8, high = 128, step = 16)

    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=25, start_from_epoch=50, baseline = 100)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)
    # random forest classifier object
    scores = []
    best_scores = []
    for _ in range(2):
        vals = list(set(group_id))
        random.shuffle(vals)

        # Calculate the index to split
        split_idx = int(0.8 * len(vals))

        # Split the list into 80% and 20%
        train = vals[:split_idx]
        test = vals[split_idx:]

        model = create_model(num_ft_layers = num_ft_layers,neurons1 = neurons1, neurons2 = neurons2) # compile model
        train = [i for i,a in enumerate(group_id) if a in train]
        test = [i for i,a in enumerate(group_id) if a in test]

        model = create_model(num_ft_layers = num_ft_layers, neurons1 = neurons1, neurons2 = neurons2)
        try:
          history = model.fit([X[train], z[train]], y[train],
                                    validation_data = ([X[test], z[test]],y[test]), epochs=100, batch_size=batch_size, callbacks= [callback, reduce_lr]).history
          best_scores.append(min(history['val_loss']))
        except:
          traceback.print_exc()
          return(2000)
    the_best_score.append(np.mean(best_scores))
    return(np.mean(best_scores))


the_best_score = []
# create a study (aim to maximize score)
study = optuna.create_study(sampler=TPESampler(), direction='minimize')

# perform hyperparamter tuning (while timing the process)
time_start = time.time()
study.optimize(objective, n_trials=3, gc_after_trial=True)
time_bayesian = time.time() - time_start

values = [study.trials[i].values[0] for i in range(len(study.trials))]
params = [study.trials[i].params for i in range(len(study.trials))]
columns = list(params[0].keys()) + ['val']
bayesian_results = pd.DataFrame(columns = columns)
bayesian_results['val'] = values
for param in list(params[0].keys()):
  param_list = [study[param] for study in params]
  bayesian_results[param] = param_list

bayesian_results.sort_values(by=['val'])

optuna.visualization.plot_param_importances(study)

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, start_from_epoch=10, baseline = 50)
from keras.callbacks import ReduceLROnPlateau
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)
model = create_model(0,162,34)
history = model.fit([bae_2d_X_train_vgg, bae_2d_z_train], bae_2d_y_train, validation_data= ([bae_2d_X_test_vgg, bae_2d_z_test], bae_2d_y_test), epochs = 100, batch_size = 88, callbacks = [callback, reduce_lr]).history

# Access the loss values from the training history
train_loss = history['loss']
val_loss = history['val_loss']

# Access the number of epochs
epochs = range(1, len(train_loss) + 1)

# Plot the training and validation curves
plt.plot(epochs, train_loss, 'b', label='Training Loss')
plt.plot(epochs, val_loss, 'r', label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

epochs = range(10, len(train_loss) + 1)

# Plot the training and validation curves
plt.plot(epochs, train_loss[9:], 'b', label='Training Loss')
plt.plot(epochs, val_loss[9:], 'r', label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

predictions = model.predict([bae_2d_X_test_vgg, bae_2d_z_test])

color = {1:'orange',-1:'blue'}
label = {1:'Male',-1:'Female'}
#plt.scatter(predictions, y_test, c=[color[i] for i in sex_test], label='Data points', label = 'a')
plt.plot([60,90], [60,90], c='green', label='x=y')

for g in [-1,1]:
    i = np.where(bae_3d_sexes_test == g)
    plt.scatter(predictions[i], bae_3d_ages_test[i], c=color[g], label = label[g])

# Add labels and title
plt.xlabel('Predicted Age')
plt.ylabel('Actual Age')
plt.title('Scatter Plot of Predictions vs. Actual Age')

# Add a legend
plt.legend()

# Show the plot
plt.show()

import numpy as np
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, confusion_matrix
from scipy.stats import pearsonr

def evaluate_predictions(predictions, true_values):
    # Calculate R^2
    r2 = r2_score(true_values, predictions)

    # Calculate MSE
    mse = mean_squared_error(true_values, predictions)

    # Calculate RMSE
    rmse = np.sqrt(mse)

    # Calculate MAE
    mae = mean_absolute_error(true_values, predictions)

    # Calculate Pearson's correlation coefficient
    r, _ = pearsonr(true_values, predictions)



    return r2, rmse, mse, mae, r

# Sample data
# (You'll need to define 'predictions' and 'bae_3d_ages_test' before calling the function)

r2, rmse, mse, mae, pearson_r = evaluate_predictions([float(i) for i in predictions], bae_2d_y_test)
print(f"R^2: {r2}")
print(f"RMSE: {rmse}")
print(f"MSE: {mse}")
print(f"MAE: {mae}")
print(f"Pearson's r: {pearson_r}")

"""### Version 2"""

from keras.applications.vgg16 import VGG16
pretrained_model = VGG16(input_shape=(95, 79, 3), include_top=False)

bae_2d_X_train_vgg = np.repeat(bae_2d_X_train[:, :, :, np.newaxis], 3, axis=3)
bae_2d_X_test_vgg = np.repeat(bae_2d_X_test[:, :, :, np.newaxis], 3, axis=3)

from keras.layers import Dropout, BatchNormalization

def create_model(num_ft_layers,neurons1, neurons2):

  # Define the input layer for the 3D Convolutional network
  model = Sequential()

  for layer in pretrained_model.layers[:11]:
      model.add(layer)

  # Freeze the weights of the pre-trained layers so they are not updated during training
  for layer in model.layers:
      layer.trainable = False


  # Flatten the output of the pre-trained model
  x = Flatten()(model.output)

  additional_input = Input(shape=(1,))


  # Add Dense layers after concatenation
  x = Dense(neurons1, activation='relu')(x)

  x = Dense(neurons2, activation='relu')(x)

  x = concatenate([x, additional_input])
  # Concatenate the flattened output with the new input
  output = Dense(1, activation='linear')(x)  # Use 'linear' activation for regression tasks

  # Create the final model
  model = Model(inputs=[pretrained_model.input, additional_input], outputs=output)

  for layer in model.layers[11 - num_ft_layers:]:
            layer.trainable = True

  session = K.get_session()
  for layer in model.layers[11 - num_ft_layers:11]:
      if layer.get_weights():
        weights_shape = layer.get_weights()[0].shape
        bias_shape = layer.get_weights()[1].shape

        # Re-initialize weights and biases
        new_weights = GlorotUniform()(weights_shape).numpy()
        new_biases = GlorotUniform()(bias_shape).numpy()

        layer.set_weights([new_weights, new_biases])
  METRICS = [keras.metrics.MeanAbsoluteError(name='mae')]



  # Compile the model
  model.compile(loss='mean_squared_error', optimizer='adam', metrics=METRICS)

  return(model)

def objective(trial, X = bae_2d_X_train_vgg,y = bae_2d_y_train,z=bae_2d_z_train ,K = 3):
    global the_best_score
    """return the mse"""
    num_ft_layers = trial.suggest_int('num_layers', low = 0, high = 12, step = 2)
    neurons1 = trial.suggest_int('neurons1', low = 2, high = 256, step = 16)
    neurons2 = trial.suggest_int('neurons2', low = 2, high = 256, step = 16)
    batch_size = trial.suggest_int('batch_size', low = 8, high = 128, step = 16)

    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=25, start_from_epoch=50, baseline = 100)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)
    # random forest classifier object
    scores = []
    best_scores = []
    for train, test in KFold(n_splits=K, shuffle=True).split(range(81)):
        model = create_model(num_ft_layers = num_ft_layers,neurons1 = neurons1, neurons2 = neurons2) # compile model
        train = [i for i,a in enumerate(group_id) if a in train+1]
        test = [i for i,a in enumerate(group_id) if a in test+1]

        model = compile_model(num_ft_layers = num_ft_layers, neurons1 = neurons1, neurons2 = neurons2)
        try:
          history = model.fit([X[train], z[train]], y[train],
                                    validation_data = ([X[test], z[test]],y[test]), epochs=100, batch_size=batch_size, callbacks= [callback, reduce_lr]).history
          best_scores.append(min(history['val_loss']))
        except:
          traceback.print_exc()
          return(2000)
    the_best_score.append(np.mean(best_scores))
    return(np.mean(best_scores))


the_best_score = []
# create a study (aim to maximize score)
study = optuna.create_study(sampler=TPESampler(), direction='minimize')

# perform hyperparamter tuning (while timing the process)
time_start = time.time()
study.optimize(objective, n_trials=3, gc_after_trial=True)
time_bayesian = time.time() - time_start

values = [study.trials[i].values[0] for i in range(len(study.trials))]
params = [study.trials[i].params for i in range(len(study.trials))]
columns = list(params[0].keys()) + ['val']
bayesian_results = pd.DataFrame(columns = columns)
bayesian_results['val'] = values
for param in list(params[0].keys()):
  param_list = [study[param] for study in params]
  bayesian_results[param] = param_list

bayesian_results.sort_values(by=['val'])

optuna.visualization.plot_param_importances(study)

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, start_from_epoch=10, baseline = 50)
from keras.callbacks import ReduceLROnPlateau
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)
model = create_model(8,162,82)
history = model.fit([bae_2d_X_train_vgg, bae_2d_z_train], bae_2d_y_train, validation_data= ([bae_2d_X_test_vgg, bae_2d_z_test], bae_2d_y_test), epochs = 100, batch_size = 40, callbacks = [callback, reduce_lr]).history

# Access the loss values from the training history
train_loss = history['loss']
val_loss = history['val_loss']

# Access the number of epochs
epochs = range(1, len(train_loss) + 1)

# Plot the training and validation curves
plt.plot(epochs, train_loss, 'b', label='Training Loss')
plt.plot(epochs, val_loss, 'r', label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

epochs = range(10, len(train_loss) + 1)

# Plot the training and validation curves
plt.plot(epochs, train_loss[9:], 'b', label='Training Loss')
plt.plot(epochs, val_loss[9:], 'r', label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

predictions = model.predict([bae_2d_X_test_vgg, bae_2d_z_test])

color = {1:'orange',-1:'blue'}
label = {1:'Male',-1:'Female'}
#plt.scatter(predictions, y_test, c=[color[i] for i in sex_test], label='Data points', label = 'a')
plt.plot([60,90], [60,90], c='green', label='x=y')

for g in [-1,1]:
    i = np.where(bae_3d_sexes_test == g)
    plt.scatter(predictions[i], bae_3d_ages_test[i], c=color[g], label = label[g])

# Add labels and title
plt.xlabel('Predicted Age')
plt.ylabel('Actual Age')
plt.title('Scatter Plot of Predictions vs. Actual Age')

# Add a legend
plt.legend()

# Show the plot
plt.show()

import numpy as np
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, confusion_matrix
from scipy.stats import pearsonr

def evaluate_predictions(predictions, true_values):
    # Calculate R^2
    r2 = r2_score(true_values, predictions)

    # Calculate MSE
    mse = mean_squared_error(true_values, predictions)

    # Calculate RMSE
    rmse = np.sqrt(mse)

    # Calculate MAE
    mae = mean_absolute_error(true_values, predictions)

    # Calculate Pearson's correlation coefficient
    r, _ = pearsonr(true_values, predictions)



    return r2, rmse, mse, mae, r

# Sample data
# (You'll need to define 'predictions' and 'bae_3d_ages_test' before calling the function)

r2, rmse, mse, mae, pearson_r = evaluate_predictions([float(i) for i in predictions], bae_2d_y_test)
print(f"R^2: {r2}")
print(f"RMSE: {rmse}")
print(f"MSE: {mse}")
print(f"MAE: {mae}")
print(f"Pearson's r: {pearson_r}")

"""### Version 2?"""

from keras.applications.vgg16 import VGG16
pretrained_model = VGG16(input_shape=(95, 79, 3), include_top=False)

bae_2d_X_train_vgg = np.repeat(bae_2d_X_train[:, :, :, np.newaxis], 3, axis=3)
bae_2d_X_test_vgg = np.repeat(bae_2d_X_test[:, :, :, np.newaxis], 3, axis=3)

def compile_model():

  # Define the input layer for the 3D Convolutional network
  model = Sequential()

  for layer in pretrained_model.layers:
      model.add(layer)

  # Freeze the weights of the pre-trained layers so they are not updated during training
  for layer in model.layers:
      layer.trainable = False


  # Flatten the output of the pre-trained model
  x = Flatten()(model.output)

  additional_input = Input(shape=(1,))


  # Add Dense layers after concatenation
  x = Dense(256, activation='relu')(x)

  x = concatenate([x, additional_input])

  x = Dense(64, activation='relu')(x)

  # Concatenate the flattened output with the new input
  output = Dense(1, activation='linear')(x)  # Use 'linear' activation for regression tasks

  # Create the final model
  model = Model(inputs=[pretrained_model.input, additional_input], outputs=output)

  for layer in model.layers[16:]:
            layer.trainable = True

  METRICS = [keras.metrics.MeanAbsoluteError(name='mae')]

  # Compile the model
  model.compile(loss='mean_squared_error', optimizer='adam', metrics=METRICS)

  return(model)

model = compile_model()
history = model.fit([bae_2d_X_train_vgg, bae_2d_z_train], bae_2d_y_train, validation_data= ([bae_2d_X_test_vgg, bae_2d_z_test], bae_2d_y_test), epochs = 100, batch_size = 16).history

predictions = model.predict([bae_2d_X_test_vgg, bae_2d_z_test])

from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

def evaluate_predictions(predictions, true_values):
    # Calculate R^2
    r2 = r2_score(true_values, predictions)

    # Calculate MSE
    mse = mean_squared_error(true_values, predictions)

    # Calculate RMSE
    rmse = np.sqrt(mse)

    # Calculate MAE
    mae = mean_absolute_error(true_values, predictions)

    return r2, rmse, mse, mae

# Sample data


r2, rmse, mse, mae = evaluate_predictions(predictions, bae_2d_y_test)
print(f"R^2: {r2}")
print(f"RMSE: {rmse}")
print(f"MSE: {mse}")
print(f"MAE: {mae}")

color = {1:'orange',-1:'blue'}
label = {1:'Male',-1:'Female'}
#plt.scatter(predictions, y_test, c=[color[i] for i in sex_test], label='Data points', label = 'a')
plt.plot([60,90], [60,90], c='green', label='x=y')

for g in [-1,1]:
    i = np.where(bae_2d_z_test == g)
    plt.scatter(predictions[i], bae_2d_y_test[i], c=color[g], label = label[g])

# Add labels and title
plt.xlabel('Predicted Age')
plt.ylabel('Actual Age')
plt.title('Scatter Plot of Predictions vs. Actual Age')

# Add a legend
plt.legend()

# Show the plot
plt.show()

"""### Version 5"""

from keras.applications.vgg19 import VGG19
pretrained_model = VGG19(input_shape=(95, 79, 3), include_top=False)

bae_2d_X_train_vgg = np.repeat(bae_2d_X_train[:, :, :, np.newaxis], 3, axis=3)
bae_2d_X_test_vgg = np.repeat(bae_2d_X_test[:, :, :, np.newaxis], 3, axis=3)

def compile_model():

  # Define the input layer for the 3D Convolutional network
  model = Sequential()

  for layer in pretrained_model.layers:
      model.add(layer)

  # Freeze the weights of the pre-trained layers so they are not updated during training
  for layer in model.layers:
      layer.trainable = False


  # Flatten the output of the pre-trained model
  x = Flatten()(model.output)

  additional_input = Input(shape=(1,))


  # Add Dense layers after concatenation
  x = Dense(256, activation='relu')(x)

  x = concatenate([x, additional_input])

  x = Dense(64, activation='relu')(x)

  # Concatenate the flattened output with the new input
  output = Dense(1, activation='linear')(x)  # Use 'linear' activation for regression tasks

  # Create the final model
  model = Model(inputs=[pretrained_model.input, additional_input], outputs=output)

  for layer in model.layers[22:]:
            layer.trainable = True

  METRICS = [keras.metrics.MeanAbsoluteError(name='mae')]

  # Compile the model
  model.compile(loss='mean_squared_error', optimizer='adam', metrics=METRICS)

  return(model)

model = compile_model()
history = model.fit([bae_2d_X_train_vgg, bae_2d_z_train], bae_2d_y_train, validation_data= ([bae_2d_X_test_vgg, bae_2d_z_test], bae_2d_y_test), epochs = 200, batch_size = 16).history
print(model.evaluate([bae_2d_X_test_vgg, bae_2d_z_test], bae_2d_y_test))

"""### Version 3"""

from keras.applications.vgg19 import VGG19
pretrained_model = VGG19(input_shape=(95, 79, 3), include_top=False)

bae_2d_X_train_ = np.array(twod_df[twod_df.split == 'train']['data'])
bae_2d_y_train_ = np.array(twod_df[twod_df.split == 'train']['age'])
bae_2d_X_test_ = np.array(twod_df[twod_df.split == 'test']['data'])
bae_2d_y_test_ = np.array(twod_df[twod_df.split == 'test']['age'])

bae_2d_X_train_ = np.array([bae_2d_X_train_[i] for i in range(len(bae_2d_X_train_))])
bae_2d_y_train_ = np.array([bae_2d_y_train_[i] for i in range(len(bae_2d_y_train_))])
bae_2d_X_test_ = np.array([bae_2d_X_test_[i] for i in range(len(bae_2d_X_test_))])
bae_2d_y_test_ = np.array([bae_2d_y_test_[i] for i in range(len(bae_2d_y_test_))])

bae_2d_z_train_ =  np.array(twod_df[twod_df.split == 'train']['sex'])
bae_2d_z_test_ =  np.array(twod_df[twod_df.split == 'test']['sex'])
bae_2d_z_train_ = np.array([bae_2d_z_train_[i] for i in range(len(bae_2d_z_train_))])
bae_2d_z_test_ = np.array([bae_2d_z_test_[i] for i in range(len(bae_2d_z_test_))])

from keras.applications.vgg19 import VGG19
pretrained_model = VGG19(input_shape=(95, 79, 3), include_top=False)

bae_2d_X_train_vgg_ = np.repeat(bae_2d_X_train_[:, :, :, np.newaxis], 3, axis=3)
bae_2d_X_test_vgg_ = np.repeat(bae_2d_X_test_[:, :, :, np.newaxis], 3, axis=3)

group_id_ = np.array(twod_df[twod_df.split == 'train']['id'])

def create_model(num_ft_layers,neurons1, neurons2):

  # Define the input layer for the 3D Convolutional network
  model = Sequential()

  for layer in pretrained_model.layers[:14]:
      model.add(layer)

  # Freeze the weights of the pre-trained layers so they are not updated during training
  for layer in model.layers:
      layer.trainable = False


  # Flatten the output of the pre-trained model
  x = Flatten()(model.output)

  additional_input = Input(shape=(1,))


  # Add Dense layers after concatenation
  x = Dense(neurons1, activation='relu')(x)
  x = Dense(neurons2, activation='relu')(x)
  x = concatenate([x, additional_input])
  # Concatenate the flattened output with the new input
  output = Dense(1, activation='linear')(x)  # Use 'linear' activation for regression tasks

  # Create the final model
  model = Model(inputs=[pretrained_model.input, additional_input], outputs=output)

  for layer in model.layers[14 - num_ft_layers:]:
            layer.trainable = True

  session = K.get_session()
  for layer in model.layers[14 - num_ft_layers:14]:
      if layer.get_weights():
        weights_shape = layer.get_weights()[0].shape
        bias_shape = layer.get_weights()[1].shape

        # Re-initialize weights and biases
        new_weights = GlorotUniform()(weights_shape).numpy()
        new_biases = GlorotUniform()(bias_shape).numpy()

        layer.set_weights([new_weights, new_biases])
  METRICS = [keras.metrics.MeanAbsoluteError(name='mae')]



  # Compile the model
  model.compile(loss='mean_squared_error', optimizer='adam', metrics=METRICS)

  return(model)

def objective(trial, X = bae_2d_X_train_vgg_,y = bae_2d_y_train_,z=bae_2d_z_train_ ,K = 3):
    global the_best_score
    """return the mse"""
    num_ft_layers = trial.suggest_int('num_layers', low = 0, high = 12, step = 2)
    neurons1 = trial.suggest_int('neurons1', low = 2, high = 256, step = 16)
    neurons2 = trial.suggest_int('neurons2', low = 2, high = 256, step = 16)
    batch_size = trial.suggest_int('batch_size', low = 8, high = 128, step = 16)

    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=25, start_from_epoch=50, baseline = 100)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)
    # random forest classifier object
    scores = []
    best_scores = []
    for _ in range(2):
        vals = list(set(group_id_))
        random.shuffle(vals)

        # Calculate the index to split
        split_idx = int(0.8 * len(vals))

        # Split the list into 80% and 20%
        train = vals[:split_idx]
        test = vals[split_idx:]

        model = create_model(num_ft_layers = num_ft_layers,neurons1 = neurons1, neurons2 = neurons2) # compile model
        train = [i for i,a in enumerate(group_id) if a in train]
        test = [i for i,a in enumerate(group_id) if a in test]


        model = create_model(num_ft_layers = num_ft_layers, neurons1 = neurons1, neurons2 = neurons2)
        try:
          history = model.fit([X[train], z[train]], y[train],
                                    validation_data = ([X[test], z[test]],y[test]), epochs=100, batch_size=batch_size, callbacks= [callback, reduce_lr]).history
          best_scores.append(min(history['val_loss']))
        except:
          traceback.print_exc()
          return(2000)
    the_best_score.append(np.mean(best_scores))
    return(np.mean(best_scores))


the_best_score = []
# create a study (aim to maximize score)
study = optuna.create_study(sampler=TPESampler(), direction='minimize')

# perform hyperparamter tuning (while timing the process)
time_start = time.time()
study.optimize(objective, n_trials=3, gc_after_trial=True)
time_bayesian = time.time() - time_start

values = [study.trials[i].values[0] for i in range(len(study.trials))]
params = [study.trials[i].params for i in range(len(study.trials))]
columns = list(params[0].keys()) + ['val']
bayesian_results = pd.DataFrame(columns = columns)
bayesian_results['val'] = values
for param in list(params[0].keys()):
  param_list = [study[param] for study in params]
  bayesian_results[param] = param_list

bayesian_results.sort_values(by=['val'])

optuna.visualization.plot_param_importances(study)

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, start_from_epoch=10, baseline = 50)
from keras.callbacks import ReduceLROnPlateau
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)
model = create_model(6,194,98)
history = model.fit([bae_2d_X_train_vgg_, bae_2d_z_train_], bae_2d_y_train_, validation_data= ([bae_2d_X_test_vgg_, bae_2d_z_test_], bae_2d_y_test_), epochs = 100, batch_size = 104, callbacks = [callback, reduce_lr]).history

# Access the loss values from the training history
train_loss = history['loss']
val_loss = history['val_loss']

# Access the number of epochs
epochs = range(1, len(train_loss) + 1)

# Plot the training and validation curves
plt.plot(epochs, train_loss, 'b', label='Training Loss')
plt.plot(epochs, val_loss, 'r', label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

epochs = range(10, len(train_loss) + 1)

# Plot the training and validation curves
plt.plot(epochs, train_loss[9:], 'b', label='Training Loss')
plt.plot(epochs, val_loss[9:], 'r', label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

predictions = model.predict([bae_2d_X_test_vgg_, bae_2d_z_test_])

color = {1:'orange',-1:'blue'}
label = {1:'Male',-1:'Female'}
#plt.scatter(predictions, y_test, c=[color[i] for i in sex_test], label='Data points', label = 'a')
plt.plot([60,90], [60,90], c='green', label='x=y')

for g in [-1,1]:
    i = np.where(bae_2d_z_test_ == g)
    plt.scatter(predictions[i], bae_2d_y_test_[i], c=color[g], label = label[g])

# Add labels and title
plt.xlabel('Predicted Age')
plt.ylabel('Actual Age')
plt.title('Scatter Plot of Predictions vs. Actual Age')

# Add a legend
plt.legend()

# Show the plot
plt.show()

import numpy as np
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, confusion_matrix
from scipy.stats import pearsonr

def evaluate_predictions(predictions, true_values):
    # Calculate R^2
    r2 = r2_score(true_values, predictions)

    # Calculate MSE
    mse = mean_squared_error(true_values, predictions)

    # Calculate RMSE
    rmse = np.sqrt(mse)

    # Calculate MAE
    mae = mean_absolute_error(true_values, predictions)

    # Calculate Pearson's correlation coefficient
    r, _ = pearsonr(true_values, predictions)



    return r2, rmse, mse, mae, r

# Sample data
# (You'll need to define 'predictions' and 'bae_3d_ages_test' before calling the function)

r2, rmse, mse, mae, pearson_r = evaluate_predictions([float(i) for i in predictions], bae_2d_y_test_)
print(f"R^2: {r2}")
print(f"RMSE: {rmse}")
print(f"MSE: {mse}")
print(f"MAE: {mae}")
print(f"Pearson's r: {pearson_r}")

"""### Version 4"""

from keras.applications.vgg16 import VGG16
pretrained_model = VGG16(input_shape=(95, 79, 3), include_top=False)

bae_2d_X_train_ = np.array(twod_df[twod_df.split == 'train']['data'])
bae_2d_y_train_ = np.array(twod_df[twod_df.split == 'train']['age'])
bae_2d_X_test_ = np.array(twod_df[twod_df.split == 'test']['data'])
bae_2d_y_test_ = np.array(twod_df[twod_df.split == 'test']['age'])

bae_2d_X_train_ = np.array([bae_2d_X_train_[i] for i in range(len(bae_2d_X_train_))])
bae_2d_y_train_ = np.array([bae_2d_y_train_[i] for i in range(len(bae_2d_y_train_))])
bae_2d_X_test_ = np.array([bae_2d_X_test_[i] for i in range(len(bae_2d_X_test_))])
bae_2d_y_test_ = np.array([bae_2d_y_test_[i] for i in range(len(bae_2d_y_test_))])

bae_2d_z_train_ =  np.array(twod_df[twod_df.split == 'train']['sex'])
bae_2d_z_test_ =  np.array(twod_df[twod_df.split == 'test']['sex'])
bae_2d_z_train_ = np.array([bae_2d_z_train_[i] for i in range(len(bae_2d_z_train_))])
bae_2d_z_test_ = np.array([bae_2d_z_test_[i] for i in range(len(bae_2d_z_test_))])

bae_2d_X_train_vgg_ = np.repeat(bae_2d_X_train_[:, :, :, np.newaxis], 3, axis=3)
bae_2d_X_test_vgg_ = np.repeat(bae_2d_X_test_[:, :, :, np.newaxis], 3, axis=3)

group_id_ = np.array(twod_df[twod_df.split == 'train']['id'])

def create_model(num_ft_layers,neurons1, neurons2):

  # Define the input layer for the 3D Convolutional network
  model = Sequential()

  for layer in pretrained_model.layers[:14]:
      model.add(layer)

  # Freeze the weights of the pre-trained layers so they are not updated during training
  for layer in model.layers:
      layer.trainable = False


  # Flatten the output of the pre-trained model
  x = Flatten()(model.output)

  additional_input = Input(shape=(1,))


  # Add Dense layers after concatenation
  x = Dense(neurons1, activation='relu')(x)
  x = Dense(neurons2, activation='relu')(x)
  x = concatenate([x, additional_input])
  # Concatenate the flattened output with the new input
  output = Dense(1, activation='linear')(x)  # Use 'linear' activation for regression tasks

  # Create the final model
  model = Model(inputs=[pretrained_model.input, additional_input], outputs=output)

  for layer in model.layers[14 - num_ft_layers:]:
            layer.trainable = True

  session = K.get_session()
  for layer in model.layers[14 - num_ft_layers:14]:
      if layer.get_weights():
        weights_shape = layer.get_weights()[0].shape
        bias_shape = layer.get_weights()[1].shape

        # Re-initialize weights and biases
        new_weights = GlorotUniform()(weights_shape).numpy()
        new_biases = GlorotUniform()(bias_shape).numpy()

        layer.set_weights([new_weights, new_biases])
  METRICS = [keras.metrics.MeanAbsoluteError(name='mae')]



  # Compile the model
  model.compile(loss='mean_squared_error', optimizer='adam', metrics=METRICS)

  return(model)

group_id_

import random
def objective(trial, X = bae_2d_X_train_vgg_,y = bae_2d_y_train_,z=bae_2d_z_train_ ,K = 3):
    global the_best_score
    """return the mse"""
    num_ft_layers = trial.suggest_int('num_layers', low = 0, high = 12, step = 2)
    neurons1 = trial.suggest_int('neurons1', low = 2, high = 256, step = 16)
    neurons2 = trial.suggest_int('neurons2', low = 2, high = 256, step = 16)
    batch_size = trial.suggest_int('batch_size', low = 8, high = 128, step = 16)

    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=25, start_from_epoch=50, baseline = 100)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)
    # random forest classifier object
    scores = []
    best_scores = []
    for _ in range(2):
        vals = list(set(group_id_))
        random.shuffle(vals)

        # Calculate the index to split
        split_idx = int(0.8 * len(vals))

        # Split the list into 80% and 20%
        train = vals[:split_idx]
        test = vals[split_idx:]

        model = create_model(num_ft_layers = num_ft_layers,neurons1 = neurons1, neurons2 = neurons2) # compile model
        train = [i for i,a in enumerate(group_id) if a in train]
        test = [i for i,a in enumerate(group_id) if a in test]


        model = create_model(num_ft_layers = num_ft_layers, neurons1 = neurons1, neurons2 = neurons2)
        try:
          history = model.fit([X[train], z[train]], y[train],
                                    validation_data = ([X[test], z[test]],y[test]), epochs=100, batch_size=batch_size, callbacks= [callback, reduce_lr]).history
          best_scores.append(min(history['val_loss']))
        except:
          traceback.print_exc()
          return(2000)
    the_best_score.append(np.mean(best_scores))
    return(np.mean(best_scores))


the_best_score = []
# create a study (aim to maximize score)
study = optuna.create_study(sampler=TPESampler(), direction='minimize')

# perform hyperparamter tuning (while timing the process)
time_start = time.time()
study.optimize(objective, n_trials=3, gc_after_trial=True)
time_bayesian = time.time() - time_start

values = [study.trials[i].values[0] for i in range(len(study.trials))]
params = [study.trials[i].params for i in range(len(study.trials))]
columns = list(params[0].keys()) + ['val']
bayesian_results = pd.DataFrame(columns = columns)
bayesian_results['val'] = values
for param in list(params[0].keys()):
  param_list = [study[param] for study in params]
  bayesian_results[param] = param_list

bayesian_results.sort_values(by=['val'])

optuna.visualization.plot_param_importances(study)

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, start_from_epoch=10, baseline = 50)
from keras.callbacks import ReduceLROnPlateau
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)
model = create_model(12,18,178)
history = model.fit([bae_2d_X_train_vgg_, bae_2d_z_train_], bae_2d_y_train_, validation_data= ([bae_2d_X_test_vgg_, bae_2d_z_test_], bae_2d_y_test_), epochs = 100, batch_size = 8, callbacks = [callback, reduce_lr]).history

# Access the loss values from the training history
train_loss = history['loss']
val_loss = history['val_loss']

# Access the number of epochs
epochs = range(1, len(train_loss) + 1)

# Plot the training and validation curves
plt.plot(epochs, train_loss, 'b', label='Training Loss')
plt.plot(epochs, val_loss, 'r', label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

epochs = range(10, len(train_loss) + 1)

# Plot the training and validation curves
plt.plot(epochs, train_loss[9:], 'b', label='Training Loss')
plt.plot(epochs, val_loss[9:], 'r', label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

predictions = model.predict([bae_2d_X_test_vgg_, bae_2d_z_test_])

color = {1:'orange',-1:'blue'}
label = {1:'Male',-1:'Female'}
#plt.scatter(predictions, y_test, c=[color[i] for i in sex_test], label='Data points', label = 'a')
plt.plot([60,90], [60,90], c='green', label='x=y')

for g in [-1,1]:
    i = np.where(bae_2d_z_test_ == g)
    plt.scatter(predictions[i], bae_2d_y_test_[i], c=color[g], label = label[g])

# Add labels and title
plt.xlabel('Predicted Age')
plt.ylabel('Actual Age')
plt.title('Scatter Plot of Predictions vs. Actual Age')

# Add a legend
plt.legend()

# Show the plot
plt.show()

import numpy as np
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, confusion_matrix
from scipy.stats import pearsonr

def evaluate_predictions(predictions, true_values):
    # Calculate R^2
    r2 = r2_score(true_values, predictions)

    # Calculate MSE
    mse = mean_squared_error(true_values, predictions)

    # Calculate RMSE
    rmse = np.sqrt(mse)

    # Calculate MAE
    mae = mean_absolute_error(true_values, predictions)

    # Calculate Pearson's correlation coefficient
    r, _ = pearsonr(true_values, predictions)



    return r2, rmse, mse, mae, r

# Sample data
# (You'll need to define 'predictions' and 'bae_3d_ages_test' before calling the function)

r2, rmse, mse, mae, pearson_r = evaluate_predictions([float(i) for i in predictions], bae_2d_y_test_)
print(f"R^2: {r2}")
print(f"RMSE: {rmse}")
print(f"MSE: {mse}")
print(f"MAE: {mae}")
print(f"Pearson's r: {pearson_r}")

"""## Getting the best model"""

model = Sequential() # using sequential

# Add 3D Convolutional layers with MaxPooling
model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(95, 79, 1)))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(95, 79, 1)))
model.add(MaxPooling2D(pool_size=(2, 2)))


# Flatten the output and add Dense layers for age prediction
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(1, activation='linear'))  # Use 'linear' activation for regression tasks

# Compile the model
model.compile(loss='mean_squared_error', optimizer='adam', metrics=[metrics.mean_absolute_error])

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test, z_train, z_test = train_test_split(X, y, z, test_size=0.2, random_state=42)
model.fit(X_train, y_train, epochs=400, batch_size=16, validation_data = (X_test, y_test))

predictions = model.predict(X_test)

import matplotlib.pyplot as plt

color = {1:'red',-1:'blue'}
# Plot scatter points with colors based on the third list
plt.scatter(predictions, y_test, c = [color[c] for c in z_test], label='Data Points')

# Draw the line x = y
plt.plot([min(predictions), max(predictions)], [min(y_test), max(y_test)], color='green', label='y = x')

# Add legend to the plot
plt.legend(loc='upper right')

# Display the plot
plt.show()

"""What if we average round the predictions to the nearest integer"""

# Plot scatter points with colors based on the third list
plt.scatter([round(x[0]) for x in predictions], y_test, c = [color[c] for c in z_test], label='Data Points')

# Draw the line x = y
plt.plot([min(predictions), max(predictions)], [min(y_test), max(y_test)], color='green', label='y = x')

# Add legend to the plot
plt.legend(loc='upper right')

# Display the plot
plt.show()

"""# MCI conversion Predictor

## Preparing the data
"""

mci_df = pd.read_csv('/content/drive/MyDrive/MCI DATA/MCI_processed.csv')

mci_df

mci_df['Sex'] = mci_df['Sex'].apply(lambda x: 1 if x == 'M' else -1)
mci_df['Label'] = mci_df['Label'].apply(lambda x: 1 if x == 'pMCI' else 0)

"""Lets see how are data is split depending on where we set the threshhold:"""

dates = sorted(mci_df.Date.unique())
for threshhold in dates:
  print('Threshhold',threshhold)
  pMCI_counts, sMCI_counts = 0,0
  for row in mci_df.iterrows():
    if row[1]['Label'] == 1:
      if row[1]['Date'] > threshhold:
        sMCI_counts += 1
      else:
        pMCI_counts += 1
    else:
      if row[1]['Date'] < threshhold:
        None
      else:
        sMCI_counts += 1
  print(f'pMCI count: {pMCI_counts}, sMCI count: {sMCI_counts}, we removed {237 - (sMCI_counts + pMCI_counts)} images')

"""36 seems to be the best threshhold as it splits the data the best without removing too much data"""

mci_df['Golden Label'] = ''
for i, row in enumerate(mci_df.iterrows()):
    if row[1]['Label'] == 1:
      if row[1]['Date'] > 36:
        mci_df.loc[i, 'Golden Label'] = 'sMCI'

      else:
        mci_df.loc[i, 'Golden Label'] = 'pMCI'
    else:
      if row[1]['Date'] < 36:
        None
      else:
        mci_df.loc[i, 'Golden Label'] = 'sMCI'

mci_df= mci_df[mci_df['Golden Label'] != '']

mci_df['Golden Label'] = mci_df['Golden Label'].apply(lambda x: 1 if x == 'pMCI' else 0)

mci_dates = []
for row in mci_df.iterrows():
  if row[1]['Golden Label'] == 0:
    mci_dates.append(1000)
  else:
    res = row[1]['Date']
    mci_dates.append(res)

# new colunm for the paths
mci_df['Loc'] = ''
directory = '/content/drive/MyDrive/MCI DATA/output_imageID' # enter the dir were the images are stored

# iterate over files in that directory
for filename in os.listdir(directory):
    f = os.path.join(directory, filename)
    image_id_folder = filename
    if image_id_folder in list(mci_df['Image Data ID']):
      for image in os.listdir(f):
        f = os.path.join(f, image)
        if os.path.isfile(f) and f.endswith('nii'):  # checking if it is a file and a .nii file
            mci_df.loc[mci_df['Image Data ID']==image_id_folder, 'Loc' ] = f # appending the path

mci_df[mci_df['Loc'] == '']

# Extract image data and corresponding ages from the DataFrame
mci_image_paths = mci_df['Loc'].values
mci_ages = mci_df['Age'].values
mci_sexes = mci_df['Sex'].values
mci_labels = mci_df['Golden Label'].values

# Initialize empty lists to store image arrays and labels
mci_image_data = []

# Loop through each image path, load the image, and append to the lists
for path in mci_image_paths:
    image = nib.load(path).get_fdata()
    mci_image_data.append(image)

# Convert lists to numpy arrays
mci_image_data = np.array(mci_image_data)

count = 0
locations = []
shape = mci_image_data.shape
for a in range(shape[0]):
  for b in range(shape[1]):
    for c in range(shape[2]):
      for d in range(shape[3]):
        if pd.isna(mci_image_data[a][b][c][d]):
          locations.append([a,b,c,d])
          count+=1
print(count)

from collections import Counter
Counter([ele[0] for ele in locations])

plt.imshow(mci_image_data[9][20], cmap='bone')
plt.axis('off')
plt.show()

for location in locations:
  surrounding_pixels = []
  try:
    surrounding_pixels.append(mci_image_data[location[0]][location[1]-1][location[2]][location[3]])
  except:
    None
  try:
    surrounding_pixels.append(mci_image_data[location[0]][location[1]+1][location[2]][location[3]])
  except:
    None
  try:
    surrounding_pixels.append(mci_image_data[location[0]][location[1]][location[2]-1][location[3]])
  except:
    None
  try:
    surrounding_pixels.append(mci_image_data[location[0]][location[1]][location[2]+1][location[3]])
  except:
    None
  try:
    surrounding_pixels.append(mci_image_data[location[0]][location[1]][location[2]][location[3]-1])
  except:
    None
  try:
    surrounding_pixels.append(mci_image_data[location[0]][location[1]][location[2]][location[3]+1])
  except:
    None
  mci_image_data[location[0]][location[1]][location[2]][location[3]]  =  np.nanmean(surrounding_pixels)

plt.imshow(mci_image_data[9][20], cmap='bone')
plt.axis('off')
plt.show()

"""count = 0
locations = []
shape = mci_image_data.shape
for a in range(shape[0]):
  for b in range(shape[1]):
    for c in range(shape[2]):
      for d in range(shape[3]):
        if pd.isna(mci_image_data[a][b][c][d]):
          locations.append([a,b,c,d])
          count+=1
print(count)"""

# Convert mci_labels and mci_sexes to string type
mci_labels_str = [str(i) for i in mci_labels]
mci_sexes_str = [str(i) for i in mci_sexes]

# Create a combined stratification variable
stratify_var = [label + sex for label, sex in zip(mci_labels_str, mci_sexes_str)]

# Split the data using the combined stratification variable
train_idx, test_idx, a, b = train_test_split(
    np.arange(188), mci_labels, test_size=0.2, stratify=stratify_var
)

from skimage.feature import greycomatrix
from skimage import img_as_ubyte
mci_twod_df = pd.DataFrame(columns = ['id','age','sex','entropy','data', 'label'])
slice_counter = 0
rotation_angle = 90
counter = 0
for image_array, age, sex, label in zip(mci_image_data, mci_ages, mci_sexes, mci_labels):
  print(image_array.shape)
  counter += 1
  total_volumes = 1
  total_slices = image_array.shape[2]
  for current_volume in range(0, total_volumes):
      # iterate through slices
      for current_slice in range(0, total_slices):
          if (slice_counter % 1) == 0:
              if rotation_angle == 90:
                  data = np.rot90(image_array[:, :, current_slice])
              elif rotation_angle == 180:
                  data = np.rot90(np.rot90(image_array[:, :, current_slice]))
              elif rotation_angle == 270:
                  data = np.rot90(np.rot90(np.rot90(image_array[:, :, current_slice])))
              #alternate slices and save as png
              if (slice_counter % 1) == 0:
                  print('Adding image...')
                  min_val = np.min(data)
                  max_val = np.max(data)
                  im = 2 * (data - min_val) / (max_val - min_val) - 1
                  im = img_as_ubyte(im)
                  glcm = np.squeeze(greycomatrix(im, distances=[1], angles=[0], symmetric=True, normed=True))
                  entropy = -np.sum(glcm*np.log2(glcm + (glcm==0)))
                  new_row = {'id':counter, 'age':age, 'sex':sex, 'entropy':entropy, 'data':data, 'label':label}
                  mci_twod_df = mci_twod_df.append(new_row, ignore_index=True)
                  #imsave(imge_folder +str(counter)+'_'+x_label2+'.jpg', data,format='jpg')
                  print('Saved.')

plt.imshow(mci_twod_df['data'][mci_twod_df['entropy'].idxmax()], cmap='bone')
plt.axis('off')
plt.show()

plt.imshow(mci_twod_df['data'][mci_twod_df['entropy'].idxmin()], cmap='bone')
plt.axis('off')
plt.show()

"""## Some visualisations of the data"""

sex_counts = mci_df["Sex"].value_counts()
# Plot the bar chart
plt.bar(sex_counts.index, sex_counts.values)
plt.xlabel("Sex")
plt.ylabel("Number of Individuals")
plt.title("Number of Males and Females")
plt.show()

sex_counts = mci_df[mci_df['Label']==1]["Sex"].value_counts()
# Plot the bar chart
plt.bar(sex_counts.index, sex_counts.values)
plt.xlabel("Sex")
plt.ylabel("Number of Individuals")
plt.title("Number of Males and Females")
plt.show()

sex_counts = mci_df[mci_df['Label']==-1]["Sex"].value_counts()
# Plot the bar chart
plt.bar(sex_counts.index, sex_counts.values)
plt.xlabel("Sex")
plt.ylabel("Number of Individuals")
plt.title("Number of Males and Females")
plt.show()

plt.hist(mci_df["Age"], bins=20, edgecolor="black")
plt.xlabel("Age")
plt.ylabel("Frequency")
num_ticks = 11  # The number of ticks you want
x_ticks = np.linspace(mci_df["Age"].min(), mci_df["Age"].max(), num_ticks)
plt.xticks(x_ticks, fontsize=8)

plt.title("Age Distribution")

plt.hist(mci_df[mci_df['Label']==1]["Age"], bins=15, alpha=0.5, edgecolor="black", color='red')
plt.hist(mci_df[mci_df['Label']==-1]["Age"], bins=15, alpha=0.5, edgecolor="black", color = 'blue')
plt.hist(df["Age"], bins=15, alpha=0.5, edgecolor="black", color='green')
plt.xlabel("Age")
plt.ylabel("Frequency")
plt.legend(loc='upper right')

num_ticks = 11  # The number of ticks you want
x_ticks = np.linspace(mci_df["Age"].min(), mci_df["Age"].max(), num_ticks)
plt.xticks(x_ticks, fontsize=8)

plt.title("Age Distribution")

mci_twod_df

mci_high_entropy_df = pd.DataFrame(columns = ['id','age','sex','entropy','data', 'label'])
for id in mci_twod_df.id.unique():
  filtered_df = mci_twod_df[mci_twod_df.id == id].sort_values(by=['entropy'], ascending=False).head(20)
  mci_high_entropy_df = mci_high_entropy_df.append(filtered_df)

mci_high_entropy_df

mci_X = np.array(mci_high_entropy_df['data'])
mci_y = np.array(mci_high_entropy_df['label'])

mci_X = np.array([mci_X[i] for i in range(len(mci_X))])
mci_y = np.array([mci_y[i] for i in range(len(mci_y))])

mci_age = np.array(mci_high_entropy_df['age'])
mci_sex = np.array(mci_high_entropy_df['sex'])

"""## Adding the Age gap biomarker"""

mci_X_vgg = np.repeat(mci_image_data[..., np.newaxis], 3, axis=-1)

mci_age_predictions = []
for i in range(len(mci_X_vgg)):
  preds = model.predict([mci_X_vgg[i:i+1], mci_sexes[i:i+1]])
  mci_age_predictions.append(preds)

mci_age_predictions = [i[0][0] for i in mci_age_predictions]

mci_dates = np.array(mci_dates)

import matplotlib.pyplot as plt

# Create a color map: -1 is red and 1 is blue
colormap = {0: 'red', 1: 'blue'}
label = {1:'sMCI',0:'pMCI'}


for g in [0,1]:
    i = np.where(mci_labels == g)
    plt.scatter(np.array(mci_age_predictions)[i], np.array(mci_ages)[i], c=colormap[g], label = label[g])

# Plot scatter points with colors based on the third list
#plt.scatter(mci_age_predictions, mci_ages, c=[colormap[c] for c in mci_sexes], label='Data Points')

# Draw the line x = y
plt.plot([55,90], [55,90], color='green', label='y = x')

# Add legend to the plot
plt.legend(loc='upper right')

# Display the plot
plt.show()

color = {1:'orange',-1:'blue'}
label = {1:'Male',-1:'Female'}
#plt.scatter(predictions, y_test, c=[color[i] for i in sex_test], label='Data points', label = 'a')
plt.plot([60,90], [60,90], c='green', label='x=y')

for g in [-1,1]:
    i = np.where(bae_3d_sexes_test == g)
    plt.scatter(predictions[i], bae_3d_ages_test[i], c=color[g], label = label[g])

# Add labels and title
plt.xlabel('Predicted Age')
plt.ylabel('Actual Age')
plt.title('Scatter Plot of Predictions vs. Actual Age')

# Add a legend
plt.legend()

# Show the plot
plt.show()

import matplotlib.pyplot as plt


# Plot scatter points with colors based on the third list
plt.scatter([int(x[0]) for x in mci_age_predictions], mci_age, c=[colormap[c] for c in mci_y], label='Data Points')

# Draw the line x = y
plt.plot([min(mci_age_predictions), max(mci_age_predictions)], [min(mci_age), max(mci_age)], color='green', label='y = x')

# Add legend to the plot
plt.legend(loc='upper right')

# Display the plot
plt.show()

age_gap = [prediction - actual for prediction, actual in zip(mci_age_predictions, mci_ages)]

age_gap = np.array(age_gap)

conversion_date = []
mean_gap = []
for date in set(mci_dates):
  idx = np.where(mci_dates == date)
  age_gaps_for_date = age_gap[idx]
  conversion_date.append(date)
  mean_gap.append(np.mean(age_gaps_for_date))

conversion_date

mean_gap



import matplotlib.pyplot as plt

# Sample data
categories = ['6','12','18','24','36','sMCI']
values = [5.022404479980469, 4.097671910336143, 3.018092632293701, 3.209321975708008,  2.651763153076172, 2.2664838013825594]

# Create a bar graph
plt.bar(categories, values, color='blue')

# Add labels and title
plt.xlabel('Date of Conversion')
plt.ylabel('Age Gap')
plt.title('Bar Graph of Categories vs Values')

# Display the graph
plt.show()

smci_age_gaps = [age for age,label in zip(age_gap, mci_labels) if label ==0]
pmci_age_gaps = [age for age,label in zip(age_gap, mci_labels) if label ==1]

len(smci_age_gaps) + len(pmci_age_gaps)

np.mean(smci_age_gaps)

import matplotlib.pyplot as plt

# Create a combined dataset
box_plot_data = [[a for a in smci_age_gaps], [a for a in pmci_age_gaps]]

# Create labels for the box plots
labels = ['sMCI', 'pMCI']

# Create the box plots
plt.boxplot(box_plot_data, labels=labels)

# Set the title and y-axis label
plt.title("Box Plots of sMCI vs pMCI Age Gap")
plt.ylabel("Age Gap")

# Display the plot
plt.show()

"""## Building a model"""

import numpy as np
from keras.models import Sequential
from keras.layers import Conv3D, MaxPooling3D, BatchNormalization, Dense, Flatten, Activation
from keras.optimizers import SGD

def create_model():
    model = Sequential()

    # Number of feature channels
    num_channels = 8

    # Repeated 5 blocks
    for _ in range(4):
        # (3x3x3) Convolutional layer with stride of 1
        model.add(Conv3D(filters=num_channels, kernel_size=(3, 3, 3), strides=(1, 1, 1), padding='same', input_shape=(79, 95, 68, 1)))
        model.add(Activation('relu'))

        # Another (3x3x3) Convolutional layer with stride of 1
        model.add(Conv3D(filters=num_channels, kernel_size=(3, 3, 3), strides=(1, 1, 1), padding='same'))

        # 3D Batch-normalization layer
        model.add(BatchNormalization())

        # ReLU
        model.add(Activation('relu'))

        # (2x2x2) Max-pooling layer with stride of 2
        model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2)))

        # Double the number of feature channels for the next block
        num_channels *= 2

    # Flatten the output
    model.add(Flatten())

    # Fully connected layer for age prediction
    model.add(Dense(1))
    model.add(Activation('sigmoid'))
    sgd = SGD(learning_rate=0.01, momentum=0.9)

    model.compile(loss=keras.losses.binary_crossentropy, optimizer=sgd, metrics=['accuracy'])
    return model

from sklearn import metrics
from sklearn.metrics import r2_score
from keras import metrics

def compile_model():

  # Define the input layer for the 3D Convolutional network
  input_img = Input(shape=(95, 79, 1))

  # Apply 3D Convolutional layers with MaxPooling using the Functional API
  x = Conv2D(32, kernel_size=(3, 3), activation='relu')(input_img)
  x = MaxPooling2D(pool_size=(2, 2))(x)
  x = Conv2D(64, kernel_size=(3, 3), activation='relu')(x)
  x = MaxPooling2D(pool_size=(2, 2))(x)


  # Flatten the output and add Dense layers for age prediction
  x = Flatten()(x)
  x = Dense(128, activation='relu')(x)
  output_img = Dense(32, activation='relu')(x)

  # Define the input layer for the extra feature
  input_extra = Input(shape=(1,))

  # Concatenate the output of the Convolutional network with the extra feature
  merged = concatenate([output_img, input_extra])

  # Add Dense layers for further prediction
  merged = Dense(32, activation='relu')(merged)
  merged = Dense(4, activation='relu')(merged)
  output = Dense(1, activation='sigmoid')(merged)  # Use 'linear' activation for regression tasks

  # Define the model with both inputs and the final output
  model = Model(inputs=[input_img, input_extra], outputs=output)

  # Compile the model
  model.compile(loss=keras.losses.binary_crossentropy, optimizer='adam', metrics=['accuracy'])

  return(model)

def cross_validate(X, y, z, K = 5):
    scores = []
    histories = []
    for train, test in KFold(n_splits=K, shuffle=True).split(X,y):
        model = compile_model() # compile model
        histories.append(model.fit([X[train], z[train]], y[train],
                                   validation_data = ([X[test], z[test]],y[test]), epochs=200, batch_size=16, shuffle = True # feed in the test data for plotting
                                   ).history)
        scores.append(model.evaluate([X[test], z[test]], y[test], verbose = 0)) # evaluate the test dataset
    """print("average test MSE: ", np.asarray(scores)[:,0].mean())
    print("average test MAE: ", np.asarray(scores)[:,1].mean())"""
    print(model.summary())
    return scores, histories

mci_y = np.array([(i+1)/2 for i in mci_y])

scores_mci, histories_mci = cross_validate(mci_X, mci_y, age_gap, K = 5)

# Access the loss values from the training history
train_loss = history['loss']
val_loss = history['val_loss']

# Access the number of epochs
epochs = range(1, len(train_loss) + 1)

# Plot the training and validation curves
plt.plot(epochs, train_loss, 'b', label='Training Loss')
plt.plot(epochs, val_loss, 'r', label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

epochs = range(50, len(train_loss) + 1)

# Plot the training and validation curves
plt.plot(epochs, train_loss[49:], 'b', label='Training Loss')
plt.plot(epochs, val_loss[49:], 'r', label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

import random
random.seed(42)
def cross_validate(X, y, z, K = 5):
    scores = []
    histories = []
    for train, test in KFold(n_splits=K, shuffle=True).split(X,y):
        z_train = z[train]
        z_test = z[test]
        random.shuffle(z_train)
        random.shuffle(z_test)
        model = compile_model() # compile model
        histories.append(model.fit([X[train], z_train], y[train],
                                   validation_data = ([X[test], z_test],y[test]), epochs=200, batch_size=32, shuffle = True # feed in the test data for plotting
                                   ).history)
        scores.append(model.evaluate([X[test], z_test], y[test], verbose = 0)) # evaluate the test dataset
    """print("average test MSE: ", np.asarray(scores)[:,0].mean())
    print("average test MAE: ", np.asarray(scores)[:,1].mean())"""
    print(model.summary())
    return scores, histories

scores_mci_noag, histories_mci_noag = cross_validate(mci_X, mci_y, age_gap, K = 5)

mci_X_train = mci_image_data[train_idx]
mci_X_test = mci_image_data[test_idx]
mci_ages_train = mci_ages[train_idx]
mci_sexes_train = mci_sexes[train_idx]
mci_labels_train = mci_labels[train_idx]
mci_ages_test  = mci_ages[test_idx]
mci_sexes_test = mci_sexes[test_idx]
mci_labels_test = mci_labels[test_idx]

# Assuming you have your data for predictions, test_labels, and sex_train
# Create a scatter plot
callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, start_from_epoch=100, baseline = 50)
from keras.callbacks import ReduceLROnPlateau
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)

model = create_model()
history = model.fit(mci_X_train, mci_labels_train, validation_data = (mci_X_test, mci_labels_test), epochs = 100, batch_size = 28, callbacks = [callback, reduce_lr]).history
predictions = model.predict(bae_3d_im_test)



predictions = model.predict(mci_X_test[..., np.newaxis])

from sklearn.metrics import roc_curve

# roc curve for models
fpr1, tpr1, thresh1 = roc_curve(mci_labels_test, predictions, pos_label=1)

# roc curve for tpr = fpr
random_probs = [0 for i in range(len(mci_labels_test))]
p_fpr, p_tpr, _ = roc_curve(mci_labels_test, random_probs, pos_label=1)

from sklearn.metrics import roc_auc_score

# auc scores
auc_score1 = roc_auc_score(mci_labels_test, predictions)

print(auc_score1)

# matplotlib
import matplotlib.pyplot as plt
plt.style.use('seaborn')

# plot roc curves
plt.plot(fpr1, tpr1, linestyle='--',color='orange', label='AUC ROC = 0.6051')
plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')
# title
plt.title('ROC curve')
# x label
plt.xlabel('False Positive Rate')
# y label
plt.ylabel('True Positive rate')

plt.legend(loc='best')
plt.savefig('ROC',dpi=300)
plt.show();

from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score

for i in range(50,70):
  threshhold_ = i/100
  print(threshhold_)
  class_predictions =[]
  for pred in predictions:
    if pred<=threshhold_:
      class_predictions.append(0)
    else:
      class_predictions.append(1)
  print(accuracy_score(class_predictions, mci_labels_test))

class_predictions =[]
for pred in predictions:
    if pred<=0.59:
      class_predictions.append(0)
    else:
      class_predictions.append(1)

from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score

# Sample true labels and predicted labels
y_true = mci_labels_test
y_pred = class_predictions
# Accuracy
acc = accuracy_score(y_true, y_pred)
print(f"Accuracy: {acc:.4f}")

# Confusion Matrix
tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()

# Specificity
specificity = tn / (tn + fp)
print(f"Specificity: {specificity:.4f}")

# Recall (Sensitivity)
recall = tp / (tp + fn)
print(f"Recall: {recall:.4f}")

# F1 Score
f1 = f1_score(y_true, y_pred)
print(f"F1 Score: {f1:.4f}")

# AUC-ROC
auc_roc = roc_auc_score(y_true, y_pred)
print(f"AUC-ROC: {auc_roc:.4f}")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns

# Compute confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Plot confusion matrix
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.xticks([0.5,1.5], [0,1])
plt.yticks([0.5,1.5], [0,1], va='center')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns


# Compute confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Plot confusion matrix
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.xticks([0.5,1.5], ['sMCI', 'pMCI'])
plt.yticks([0.5,1.5], ['sMCI', 'pMCI'], va='center')
plt.show()

"""### x"""